{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b702e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tables\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# --- Global configuration ---\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "H5_PATH = \"output/dataset.h5\"\n",
    "SPLIT_CSV = \"split_all.tsv\"\n",
    "MODEL_DIR = \"modelos\"\n",
    "IMG_DIR = \"imgs\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "EMB = 231           # Embedding dimension\n",
    "FF_MULT = 2         # Feedforward multiplier\n",
    "MAX_SEQ_LEN = 2016  # Maximum input sequence length\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1000\n",
    "LR = 3e-4\n",
    "PATIENCE = 10\n",
    "MIN_DELTA = 1e-4\n",
    "SEEDS = [42, 43, 44]\n",
    "\n",
    "# --- Split dictionary ---\n",
    "df_split = pd.read_csv(SPLIT_CSV, sep='\\t')\n",
    "split_dict = {\n",
    "    split: set(df_split[df_split['split'] == split]['stay_id'].values)\n",
    "    for split in ['train', 'val', 'test']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Dataset class for windowed data stored in HDF5 ---\n",
    "class HDF5WindowDataset(Dataset):\n",
    "    def __init__(self, h5_path, split):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading window indices and corresponding stay_ids.\n",
    "\n",
    "        Args:\n",
    "            h5_path (str): Path to the HDF5 file\n",
    "            split (str): One of ['train', 'val', 'test']\n",
    "        \"\"\"\n",
    "        self.h5_path = h5_path\n",
    "        self.split = split\n",
    "\n",
    "        with tables.open_file(h5_path, mode='r') as f:\n",
    "            self.windows = f.root.patient_windows[split][:]\n",
    "            self.stay_ids = f.root.patient_windows[f\"{split}_stay_ids\"][:]\n",
    "\n",
    "        print(f\"[INFO] Loaded {len(self.windows)} windows for split: {split}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple (X, y) of time series data and binary label sequence.\n",
    "        \"\"\"\n",
    "        with tables.open_file(self.h5_path, mode='r') as f:\n",
    "            start, stop, _ = self.windows[idx]\n",
    "            x = f.root.data[self.split][start:stop]\n",
    "            y = f.root.labels[self.split][start:stop]\n",
    "        if len(x) > MAX_SEQ_LEN:\n",
    "            x = x[:MAX_SEQ_LEN]\n",
    "            y = y[:MAX_SEQ_LEN]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# --- Collate function for DataLoader (padding to equal length) ---\n",
    "def collate_fn(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    max_len = max(x.shape[0] for x in xs)\n",
    "    padded_x = torch.zeros(len(xs), max_len, xs[0].shape[1])\n",
    "    padded_y = torch.zeros(len(ys), max_len, ys[0].shape[1])\n",
    "    pad_mask = torch.ones(len(xs), max_len, dtype=torch.bool)\n",
    "    for i, (x, y) in enumerate(zip(xs, ys)):\n",
    "        padded_x[i, :x.shape[0]] = x\n",
    "        padded_y[i, :y.shape[0]] = y\n",
    "        pad_mask[i, :x.shape[0]] = False\n",
    "    return padded_x, padded_y.squeeze(-1), pad_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simple Transformer Model ---\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, EMB)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=EMB, nhead=1, dim_feedforward=EMB * FF_MULT,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(EMB, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.transformer(x, src_key_padding_mask=mask)\n",
    "        return self.classifier(x).squeeze(-1)\n",
    "\n",
    "# --- Simple LSTM Model ---\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, EMB, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(EMB, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.classifier(out).squeeze(-1)\n",
    "\n",
    "# --- Simple GRU Model ---\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, EMB, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(EMB, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        out, _ = self.gru(x)\n",
    "        return self.classifier(out).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ee0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation metrics for binary sequence classification ---\n",
    "def evaluate(model, loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model using provided DataLoader and loss function.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        loader: DataLoader for evaluation\n",
    "        criterion: Loss function (e.g. BCELoss)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with AUC, AUPRC, F1, Precision, Recall, and Loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    total_loss, total_samples = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, mask in loader:\n",
    "            x, y, mask = x.to(DEVICE), y.to(DEVICE), mask.to(DEVICE)\n",
    "            preds = model(x, mask)\n",
    "            loss = criterion(preds, y)\n",
    "            loss = loss.masked_fill(mask, 0.0)\n",
    "            valid = (~mask).sum()\n",
    "            total_loss += loss.sum().item()\n",
    "            total_samples += valid.item()\n",
    "            all_preds.extend(preds[~mask].cpu().numpy())\n",
    "            all_targets.extend(y[~mask].cpu().numpy())\n",
    "\n",
    "    metrics = {\n",
    "        'auc': roc_auc_score(all_targets, all_preds),\n",
    "        'auprc': average_precision_score(all_targets, all_preds),\n",
    "        'f1': f1_score(all_targets, np.round(all_preds)),\n",
    "        'precision': precision_score(all_targets, np.round(all_preds)),\n",
    "        'recall': recall_score(all_targets, np.round(all_preds)),\n",
    "        'loss': total_loss / total_samples\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22edbf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training function with early stopping and model checkpointing ---\n",
    "def train_and_evaluate(model_class, name, input_dim, seed):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a given model.\n",
    "\n",
    "    Args:\n",
    "        model_class: PyTorch model class to instantiate\n",
    "        name (str): Name identifier for saving model\n",
    "        input_dim (int): Number of input features\n",
    "        seed (int): Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (metrics_dict, trained_model, test_loader)\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        HDF5WindowDataset(H5_PATH, 'train'),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        HDF5WindowDataset(H5_PATH, 'val'),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        HDF5WindowDataset(H5_PATH, 'test'),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    model = model_class(input_dim).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-6)\n",
    "    criterion = nn.BCELoss(reduction='none')\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for x, y, mask in train_loader:\n",
    "            x, y, mask = x.to(DEVICE), y.to(DEVICE), mask.to(DEVICE)\n",
    "            preds = model(x, mask)\n",
    "            loss = criterion(preds, y)\n",
    "            loss = loss.masked_fill(mask, 0.0)\n",
    "            valid = (~mask).sum()\n",
    "            loss_val = loss.sum() / valid\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        val_metrics = evaluate(model, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch+1}: AUC={val_metrics['auc']:.4f}, F1={val_metrics['f1']:.4f}, AUPRC={val_metrics['auprc']:.4f}\")\n",
    "\n",
    "        if val_metrics['loss'] + MIN_DELTA < best_loss:\n",
    "            best_loss = val_metrics['loss']\n",
    "            patience_counter = 0\n",
    "            best_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    torch.save(best_state, f\"{MODEL_DIR}/{name}_seed{seed}.pt\")\n",
    "    test_metrics = evaluate(model, test_loader, criterion)\n",
    "    print(f\"{name} | Test AUC: {test_metrics['auc']:.4f}, F1: {test_metrics['f1']:.4f}\")\n",
    "    return {'model': name, 'seed': seed, **test_metrics}, model, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bb7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define model variants ---\n",
    "models = {\n",
    "    'Transformer': SimpleTransformer,\n",
    "    'LSTM': SimpleLSTM,\n",
    "    'GRU': SimpleGRU\n",
    "}\n",
    "\n",
    "results = []\n",
    "all_models = {}\n",
    "\n",
    "# --- Train and evaluate each model with multiple random seeds ---\n",
    "for name, cls in models.items():\n",
    "    for seed in SEEDS:\n",
    "        res, mod, loader = train_and_evaluate(cls, name, input_dim=620, seed=seed)\n",
    "        results.append(res)\n",
    "        all_models[f\"{name}_seed{seed}\"] = (mod, loader)\n",
    "\n",
    "# --- Export final results to CSV ---\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"metrics_completas.csv\", index=False)\n",
    "\n",
    "# --- Summary printout ---\n",
    "print(\"\\nSummary Results:\")\n",
    "print(df.groupby(\"model\")[[\"auc\", \"f1\", \"auprc\", \"precision\", \"recall\"]].mean())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
