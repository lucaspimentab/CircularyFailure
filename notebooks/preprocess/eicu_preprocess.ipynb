{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945884e2",
   "metadata": {},
   "source": [
    "# eICU Preprocess (standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf2c67",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d7686",
   "metadata": {},
   "source": [
    "### 0.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c15226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import Counter\n",
    "import tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3587318e",
   "metadata": {},
   "source": [
    "### 0.2 Paths & environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths\n",
    "BASE_PATH = '/scratch/leticia.ribeiro/dataset/eICU 2.0/'\n",
    "OUTPUT_DIR = 'outputs/eicu'\n",
    "\n",
    "patient_path   = Path(BASE_PATH) / 'patient.csv'\n",
    "vital_path     = Path(BASE_PATH) / 'vitalPeriodic.csv'\n",
    "lab_path       = Path(BASE_PATH) / 'lab.csv'\n",
    "customlab_path = Path(BASE_PATH) / 'customLab.csv'\n",
    "inf_path       = Path(BASE_PATH) / 'infusionDrug.csv'\n",
    "\n",
    "out = Path(OUTPUT_DIR) / 'preprocess' / 'raw_parquet'\n",
    "out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "parquet_vital     = out / 'parquet_vital'\n",
    "parquet_lab       = out / 'parquet_lab'\n",
    "parquet_customlab = out / 'parquet_customlab'\n",
    "parquet_inf       = out / 'parquet_inf'\n",
    "parquet_patient   = out / 'parquet_patient'\n",
    "\n",
    "print('BASE_PATH:', BASE_PATH)\n",
    "print('OUTPUT_DIR:', out.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827aa1b",
   "metadata": {},
   "source": [
    "### 0.3 Defaults / mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Features e defaults\n",
    "vital_features = [\n",
    "    'sao2','heartrate','respiration',\n",
    "    'systemicsystolic','systemicdiastolic','systemicmean'\n",
    "]\n",
    "\n",
    "# Labs de interesse (ajuste se necessario)\n",
    "PARAMETROS_IMPUTACAO = {\n",
    "    'albumin': {'default': 4.4},\n",
    "    'creatinine': {'default': 1.0},\n",
    "    'BUN': {'default': 13.0},\n",
    "    'sodium': {'default': 140.0},\n",
    "    'potassium': {'default': 4.5},\n",
    "    'bicarbonate': {'default': 26.0},\n",
    "    'calcium': {'default': 9.4},\n",
    "    'magnesium': {'default': 1.95},\n",
    "    'phosphate': {'default': 3.65},\n",
    "    'glucose': {'default': 90.0},\n",
    "    'lactate': {'default': 1.0},\n",
    "    'total protein': {'default': 7.3},\n",
    "    'Hgb': {'default': 14.0},\n",
    "    'Hct': {'default': 43.0},\n",
    "    'platelets x 1000': {'default': 250.0},\n",
    "    'MPV': {'default': 9.5},\n",
    "    'AST (SGOT)': {'default': 20.0},\n",
    "    'ALT (SGPT)': {'default': 20.0},\n",
    "    'total bilirubin': {'default': 0.6},\n",
    "    'direct bilirubin': {'default': 0.2},\n",
    "    'PT': {'default': 12.0},\n",
    "    'PTT': {'default': 32.0},\n",
    "    'fibrinogen': {'default': 300.0},\n",
    "    'CPK': {'default': 100.0},\n",
    "    'CPK-MB': {'default': 2.5},\n",
    "    'CPK-MB INDEX': {'default': 2.0},\n",
    "    'Methemoglobin': {'default': 1.0},\n",
    "    'ionized calcium': {'default': 5.0},\n",
    "    'urinary creatinine': {'default': 100.0},\n",
    "    'Ferritin': {'default': 100.0},\n",
    "    'Carboxyhemoglobin': {'default': 1.0},\n",
    "    'troponin': {'default': 0.01},\n",
    "}\n",
    "\n",
    "lab_features = list(PARAMETROS_IMPUTACAO.keys())\n",
    "lab_features_lower = [l.lower() for l in lab_features]\n",
    "\n",
    "NORMAL_VALUES = {\n",
    "    'sao2': 97.0,\n",
    "    'heartrate': 85.0,\n",
    "    'respiration': 18.0,\n",
    "    'systemicsystolic': 120.0,\n",
    "    'systemicdiastolic': 70.0,\n",
    "    'systemicmean': 85.0,\n",
    "}\n",
    "\n",
    "impute_defaults_vital = {v: NORMAL_VALUES.get(v, np.nan) for v in vital_features}\n",
    "impute_defaults_lab = {k.lower(): v['default'] for k, v in PARAMETROS_IMPUTACAO.items()}\n",
    "\n",
    "print('Vitals:', vital_features)\n",
    "print('Labs:', lab_features_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea7f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garantir que a lista de labs desejada esteja presente em `PARAMETROS_IMPUTACAO` e atualizar `lab_features`\n",
    "requested_labs = [\n",
    "    'heartrate',\n",
    "    'systemicsystolic',\n",
    "    'systemicdiastolic',\n",
    "    'systemicmean',\n",
    "    'respiration',\n",
    "    'sao2',\n",
    "    'albumin',\n",
    "    'creatinine',\n",
    "    'BUN',\n",
    "    'sodium',\n",
    "    'potassium',\n",
    "    'bicarbonate',\n",
    "    'calcium',\n",
    "    'magnesium',\n",
    "    'phosphate',\n",
    "    'glucose',\n",
    "    'lactate',\n",
    "    'total protein',\n",
    "    'Hgb',\n",
    "    'Hct',\n",
    "    'platelets x 1000',\n",
    "    'MPV',\n",
    "    'AST (SGOT)',\n",
    "    'ALT (SGPT)',\n",
    "    'total bilirubin',\n",
    "    'direct bilirubin',\n",
    "    'PT',\n",
    "    'PTT',\n",
    "    'fibrinogen',\n",
    "    'CPK',\n",
    "    'CPK-MB',\n",
    "    'CPK-MB INDEX',\n",
    "    'Methemoglobin',\n",
    "    'ionized calcium',\n",
    "    'urinary creatinine',\n",
    "    'Ferritin',\n",
    "    'Carboxyhemoglobin',\n",
    "    'troponin',\n",
    "]\n",
    "\n",
    "# separar vitais (jÃ¡ definidos em `vital_features`) e manter apenas labs\n",
    "requested_lab_only = [x for x in requested_labs if x not in vital_features]\n",
    "\n",
    "# adicionar entradas faltantes em PARAMETROS_IMPUTACAO com default NaN\n",
    "for lab in requested_lab_only:\n",
    "    if lab not in PARAMETROS_IMPUTACAO:\n",
    "        PARAMETROS_IMPUTACAO[lab] = {'default': np.nan}\n",
    "\n",
    "lab_features = list(PARAMETROS_IMPUTACAO.keys())\n",
    "lab_features_lower = [l.lower() for l in lab_features]\n",
    "impute_defaults_lab = {k.lower(): v.get('default', np.nan) for k, v in PARAMETROS_IMPUTACAO.items()}\n",
    "\n",
    "print('Updated lab_features length:', len(lab_features))\n",
    "print('Sample labs added (if any):', [lab for lab in requested_lab_only if PARAMETROS_IMPUTACAO.get(lab)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8463d",
   "metadata": {},
   "source": [
    "## 1) Preprocess pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9798f40",
   "metadata": {},
   "source": [
    "# Preprocess eICU (DuckDB)\n",
    "\n",
    "Este notebook gera os parquets base, cria batches de features e gera labels de falencia.\n",
    "Execute as celulas em ordem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a7875",
   "metadata": {},
   "source": [
    "## 0) Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c559751",
   "metadata": {},
   "source": [
    "### 0.1 Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f18fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8983bc",
   "metadata": {},
   "source": [
    "### 0.2 Paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859cf43",
   "metadata": {},
   "source": [
    "### 0.3 Defaults / lab mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216cf150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utils para labs\n",
    "LAB_ALIASES = {\n",
    "    'albumin': ['albumin', 'pre-albumin', 'prealbumin'],\n",
    "    'alt (sgpt)': ['alt (sgpt)', 'alanine aminotransferase', 'alt'],\n",
    "    'ast (sgot)': ['ast (sgot)', 'aspartate aminotransferase', 'ast'],\n",
    "    'bicarbonate': ['bicarbonate', 'co2', 'total co2', 'tco2', 'hco3'],\n",
    "    'bun': ['bun', 'urea nitrogen', 'blood urea nitrogen'],\n",
    "    'calcium': ['calcium', 'total calcium'],\n",
    "    'carboxyhemoglobin': ['carboxyhemoglobin', 'carboxy hb', 'cohb'],\n",
    "    'cpk': ['cpk', 'creatine kinase', 'ck'],\n",
    "    'cpk-mb': ['cpk-mb', 'ck-mb', 'cpk mb', 'ck mb'],\n",
    "    'cpk-mb index': ['cpk-mb index', 'ck-mb index', 'cpk mb index', 'ck mb index'],\n",
    "    'creatinine': ['creatinine', 'creatinine w gfr', 'creatinine w est gfr', 'creatinine w/ est gfr', 'creatinine w/ gfr'],\n",
    "    'direct bilirubin': ['direct bilirubin'],\n",
    "    'ferritin': ['ferritin'],\n",
    "    'fibrinogen': ['fibrinogen'],\n",
    "    'glucose': ['glucose', 'blood glucose', 'bedside glucose'],\n",
    "    'hct': ['hct', 'hematocrit'],\n",
    "    'hgb': ['hgb', 'hemoglobin'],\n",
    "    'ionized calcium': ['ionized calcium', 'free calcium', 'ica', 'i-ca'],\n",
    "    'lactate': ['lactate', 'lactic acid'],\n",
    "    'magnesium': ['magnesium'],\n",
    "    'methemoglobin': ['methemoglobin', 'met-hb', 'met hb'],\n",
    "    'mpv': ['mpv', 'mean platelet volume'],\n",
    "    'phosphate': ['phosphate', 'phosphorus'],\n",
    "    'alkaline phosphatase': ['alkaline phosphatase', 'alkaline phos', 'alkaline phos.', 'alk phos', 'alk phos.'],\n",
    "    'platelets x 1000': ['platelets x 1000', 'platelets', 'platelet'],\n",
    "    'potassium': ['potassium', 'k'],\n",
    "    'pt': ['pt', 'prothrombin time'],\n",
    "    'ptt': ['ptt', 'ptt ratio', 'partial thromboplastin time', 'aptt'],\n",
    "    'sodium': ['sodium', 'na'],\n",
    "    'total bilirubin': ['total bilirubin', 'bilirubin'],\n",
    "    'total protein': ['total protein'],\n",
    "    'urinary creatinine': ['urinary creatinine', 'urine creatinine'],\n",
    "    'troponin': ['troponin', 'troponin i', 'troponin - i', 'trop i', 'troponin t', 'troponin - t', 'trop t'],\n",
    "}\n",
    "\n",
    "# Cria set de lookup rapido (direct mapping de nome -> lab canonical)\n",
    "alias_lookup = {}\n",
    "for lab_canonical, variants in LAB_ALIASES.items():\n",
    "    for variant in variants:\n",
    "        alias_lookup[variant] = lab_canonical\n",
    "\n",
    "def normalize_labname(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x).strip().lower()\n",
    "    if s == 'phos':\n",
    "        return 'phosphate'\n",
    "    if 'alk' in s and 'phos' in s:\n",
    "        return 'alkaline phosphatase'\n",
    "    if s in alias_lookup:\n",
    "        return alias_lookup[s]\n",
    "    # tentativa por substring (para nomes parciais)\n",
    "    for lab_canonical, variants in LAB_ALIASES.items():\n",
    "        if any(v in s for v in variants):\n",
    "            return lab_canonical\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f641d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export CSV -> Parquet (DuckDB). Re-run safe.\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Validate source files exist\n",
    "for _p in [patient_path, vital_path, lab_path, customlab_path, inf_path]:\n",
    "    if not Path(_p).exists():\n",
    "        raise FileNotFoundError(f\"Missing source file: {_p}\")\n",
    "\n",
    "\n",
    "def export_if_missing(path_parquet, sql):\n",
    "    if Path(path_parquet).exists():\n",
    "        print('OK (skip):', path_parquet)\n",
    "        return\n",
    "    print('Exportando:', path_parquet)\n",
    "    con.execute(f\"COPY ({sql}) TO '{path_parquet}' (FORMAT PARQUET);\")\n",
    "    print('Done:', path_parquet)\n",
    "\n",
    "# vitalPeriodic\n",
    "export_if_missing(\n",
    "    parquet_vital,\n",
    "    f\"\"\"\n",
    "    SELECT patientunitstayid, observationoffset, temperature, sao2, heartrate,\n",
    "           respiration, systemicsystolic, systemicdiastolic, systemicmean\n",
    "    FROM read_csv_auto('{vital_path}')\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# lab.csv\n",
    "export_if_missing(\n",
    "    parquet_lab,\n",
    "    f\"\"\"\n",
    "    SELECT patientunitstayid, labresultoffset, labname, labresult\n",
    "    FROM read_csv_auto(\n",
    "        '{lab_path}',\n",
    "        sample_size=10000,\n",
    "        ignore_errors=true,\n",
    "        quote='\"',\n",
    "        escape='\"'\n",
    "    )\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# customLab.csv\n",
    "export_if_missing(\n",
    "    parquet_customlab,\n",
    "    f\"\"\"\n",
    "    SELECT patientunitstayid,\n",
    "           labotheroffset AS labresultoffset,\n",
    "           labothername   AS labname,\n",
    "           labotherresult AS labresult\n",
    "    FROM read_csv_auto(\n",
    "        '{customlab_path}',\n",
    "        sample_size=10000,\n",
    "        ignore_errors=true,\n",
    "        quote='\"',\n",
    "        escape='\"'\n",
    "    )\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# infusionDrug.csv\n",
    "export_if_missing(\n",
    "    parquet_inf,\n",
    "    f\"\"\"\n",
    "    SELECT patientunitstayid, infusionoffset, drugname\n",
    "    FROM read_csv_auto('{inf_path}')\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# patient.csv\n",
    "export_if_missing(\n",
    "    parquet_patient,\n",
    "    f\"\"\"\n",
    "    SELECT patientunitstayid, age, unitdischargeoffset, unitdischargestatus, hospitaldischargestatus\n",
    "    FROM read_csv_auto('{patient_path}')\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregar IDs validos (UTI, >= 1h, idade >= 18)\n",
    "patient = con.execute(\n",
    "    f\"SELECT patientunitstayid, age, unitdischargeoffset, unitdischargestatus, hospitaldischargestatus FROM read_parquet('{parquet_patient.as_posix()}')\"\n",
    ").fetch_df()\n",
    "\n",
    "# idade pode vir como string (ex: '> 89')\n",
    "patient['age'] = patient['age'].astype(str).str.replace('> 89', '90')\n",
    "patient['age'] = pd.to_numeric(patient['age'], errors='coerce')\n",
    "patient['unitdischargeoffset'] = pd.to_numeric(patient['unitdischargeoffset'], errors='coerce')\n",
    "\n",
    "adult = patient['age'] >= 18\n",
    "one_hour = patient['unitdischargeoffset'] >= 60\n",
    "\n",
    "valid = patient.loc[adult & one_hour].copy()\n",
    "patient_ids = valid['patientunitstayid'].dropna().astype(int).unique().tolist()\n",
    "\n",
    "print('Total stays (all):', patient['patientunitstayid'].nunique())\n",
    "print('Total stays >=1h and age>=18:', len(patient_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d64210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Time grid e imputacao\n",
    "\n",
    "def generate_timegrid_eicu(df_vitals, df_labs, step_min=5):\n",
    "    offsets_min = []\n",
    "    offsets_max = []\n",
    "\n",
    "    if not df_vitals.empty:\n",
    "        offsets_min.append(df_vitals['observationoffset'].min())\n",
    "        offsets_max.append(df_vitals['observationoffset'].max())\n",
    "\n",
    "    if not df_labs.empty:\n",
    "        offsets_min.append(df_labs['labresultoffset'].min())\n",
    "        offsets_max.append(df_labs['labresultoffset'].max())\n",
    "\n",
    "    if not offsets_min:\n",
    "        return np.array([], dtype=int)\n",
    "\n",
    "    t0_raw = min(offsets_min)\n",
    "    tmax_raw = max(offsets_max)\n",
    "\n",
    "    t0 = int(np.floor(t0_raw / step_min) * step_min)\n",
    "    tmax = int(np.ceil(tmax_raw / step_min) * step_min)\n",
    "\n",
    "    return np.arange(t0, tmax + 1, step_min, dtype=int)\n",
    "\n",
    "\n",
    "def impute_forward_fill_eicu(obs_offsets, obs_values, time_grid, global_fill=np.nan):\n",
    "    n = len(time_grid)\n",
    "    output = np.full(n, global_fill, dtype=np.float32)\n",
    "\n",
    "    last_val = np.nan\n",
    "    i_obs = 0\n",
    "\n",
    "    for i_pred, t_pred in enumerate(time_grid):\n",
    "        while i_obs < len(obs_offsets) and obs_offsets[i_obs] <= t_pred:\n",
    "            if pd.notna(obs_values[i_obs]):\n",
    "                last_val = obs_values[i_obs]\n",
    "            i_obs += 1\n",
    "\n",
    "        output[i_pred] = last_val if not np.isnan(last_val) else global_fill\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def impute_batch_eicu(df, variables, impute_defaults, time_grid, offset_col, stay_id_if_empty=None):\n",
    "    if df.empty:\n",
    "        if stay_id_if_empty is None:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        res = {\n",
    "            'patientunitstayid': [stay_id_if_empty] * len(time_grid),\n",
    "            'offset_min': time_grid\n",
    "        }\n",
    "\n",
    "        for var in variables:\n",
    "            res[var] = np.full(len(time_grid), impute_defaults.get(var, np.nan), dtype=np.float32)\n",
    "            res[f'{var}_imputed'] = np.ones(len(time_grid), dtype=int)\n",
    "\n",
    "        return pd.DataFrame(res)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for stay_id, group in df.groupby('patientunitstayid'):\n",
    "        group = group.sort_values(offset_col).reset_index(drop=True)\n",
    "\n",
    "        res = {\n",
    "            'patientunitstayid': [stay_id] * len(time_grid),\n",
    "            'offset_min': time_grid\n",
    "        }\n",
    "\n",
    "        for var in variables:\n",
    "            if var not in group.columns or group[var].dropna().empty:\n",
    "                res[var] = np.full(len(time_grid), impute_defaults.get(var, np.nan), dtype=np.float32)\n",
    "                res[f'{var}_imputed'] = np.ones(len(time_grid), dtype=int)\n",
    "                continue\n",
    "\n",
    "            obs_values = pd.to_numeric(group[var], errors='coerce').astype(float).to_numpy()\n",
    "            obs_offsets = pd.to_numeric(group[offset_col], errors='coerce').to_numpy()\n",
    "\n",
    "            pred_vals = impute_forward_fill_eicu(obs_offsets, obs_values, time_grid, impute_defaults.get(var, np.nan))\n",
    "            res[var] = pred_vals\n",
    "\n",
    "            mask = np.zeros(len(time_grid), dtype=int)\n",
    "            valid = ~pd.isna(obs_values)\n",
    "            obs_offsets_valid = obs_offsets[valid]\n",
    "            idxs = np.searchsorted(time_grid, obs_offsets_valid, side=\"left\")\n",
    "            idxs = idxs[(idxs >= 0) & (idxs < len(mask))]\n",
    "            mask[idxs] = 1\n",
    "            res[f'{var}_imputed'] = 1 - mask\n",
    "\n",
    "        results.append(pd.DataFrame(res))\n",
    "\n",
    "    return pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing (DuckDB) + vasopressor_active + falencia + mortality + engineered features (separadas)\n",
    "\n",
    "VASO_DRUGS = [\n",
    "    'dopamine','epinephrine','adrenalin','norepinephrine','norepi','levophed',\n",
    "    'phenylephrine','neosynephrine','vasopressin','dobutamine','milrinone',\n",
    "    'isoproterenol','isuprel'\n",
    "]\n",
    "\n",
    "vaso_regex = '|'.join(VASO_DRUGS)\n",
    "\n",
    "# Mortality map (ICU OR hospital discharge status)\n",
    "# 1 = Expired, 0 = Alive\n",
    "mortality_map = (\n",
    "    patient.assign(\n",
    "        mortality=(\n",
    "            (patient['unitdischargestatus'].astype(str).str.lower() == 'expired') |\n",
    "            (patient['hospitaldischargestatus'].astype(str).str.lower() == 'expired')\n",
    "        ).astype(int)\n",
    "    )\n",
    "    .set_index('patientunitstayid')['mortality']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "\n",
    "def build_vaso_start(batch_ids):\n",
    "    ids_csv = ','.join(str(i) for i in batch_ids)\n",
    "    inf = con.execute(\n",
    "        f\"SELECT patientunitstayid, infusionoffset, lower(drugname) AS drugname \"\n",
    "        f\"FROM read_parquet('{parquet_inf.as_posix()}') \"\n",
    "        f\"WHERE patientunitstayid IN ({ids_csv})\"\n",
    "    ).fetch_df()\n",
    "    if inf.empty:\n",
    "        return {}\n",
    "    inf = inf[inf['drugname'].str.contains(vaso_regex, regex=True, na=False)].copy()\n",
    "    inf['infusionoffset'] = pd.to_numeric(inf['infusionoffset'], errors='coerce')\n",
    "    inf = inf.dropna(subset=['patientunitstayid','infusionoffset'])\n",
    "    if inf.empty:\n",
    "        return {}\n",
    "    return inf.groupby('patientunitstayid')['infusionoffset'].min().to_dict()\n",
    "\n",
    "\n",
    "def engineered_features(df, var, imputed_col):\n",
    "    series = df[var].astype(float)\n",
    "    obs_mask = 1 - df[imputed_col].astype(int)\n",
    "    n_meas = int(obs_mask.sum())\n",
    "    min_v = float(series.min())\n",
    "    max_v = float(series.max())\n",
    "    mean_v = float(series.mean())\n",
    "    instab = float(series.diff().abs().sum())\n",
    "    intens = float(series.abs().mean())\n",
    "    cumul = float(series.sum())\n",
    "    return n_meas, min_v, max_v, mean_v, instab, intens, cumul\n",
    "\n",
    "\n",
    "def process_eicu_batch(batch_ids, step_min=5):\n",
    "    ids_csv = ','.join(str(i) for i in batch_ids)\n",
    "\n",
    "    vit = con.execute(\n",
    "        f\"SELECT patientunitstayid, observationoffset, {', '.join(vital_features)} \"\n",
    "        f\"FROM read_parquet('{parquet_vital.as_posix()}') \"\n",
    "        f\"WHERE patientunitstayid IN ({ids_csv})\"\n",
    "    ).fetch_df()\n",
    "\n",
    "    lab_raw = con.execute(\n",
    "        f\"SELECT patientunitstayid, labresultoffset, lower(trim(labname)) AS labname, labresult \"\n",
    "        f\"FROM read_parquet('{parquet_lab.as_posix()}') \"\n",
    "        f\"WHERE patientunitstayid IN ({ids_csv})\"\n",
    "    ).fetch_df()\n",
    "\n",
    "    custom_raw = con.execute(\n",
    "        f\"SELECT patientunitstayid, labresultoffset, lower(trim(labname)) AS labname, labresult \"\n",
    "        f\"FROM read_parquet('{parquet_customlab.as_posix()}') \"\n",
    "        f\"WHERE patientunitstayid IN ({ids_csv})\"\n",
    "    ).fetch_df()\n",
    "\n",
    "    lab = pd.concat([lab_raw, custom_raw], ignore_index=True)\n",
    "    if not lab.empty:\n",
    "        lab['labname'] = lab['labname'].map(normalize_labname)\n",
    "        lab = lab[lab['labname'].isin(lab_features_lower)]\n",
    "        # Pivot lab data from long format (labname, labresult) to wide format (one column per lab)\n",
    "        if not lab.empty:\n",
    "            lab = lab.pivot_table(\n",
    "                index=['patientunitstayid', 'labresultoffset'],\n",
    "                columns='labname',\n",
    "                values='labresult',\n",
    "                aggfunc='first'\n",
    "            ).reset_index()\n",
    "            lab.columns.name = None\n",
    "\n",
    "    vaso_start = build_vaso_start(batch_ids)\n",
    "\n",
    "    merged_data = []\n",
    "    engineered_rows = []\n",
    "    engineered_report = []\n",
    "    for sid in batch_ids:\n",
    "        df_v = vit[vit['patientunitstayid'] == sid]\n",
    "        df_l = lab[lab['patientunitstayid'] == sid]\n",
    "        if df_v.empty and df_l.empty:\n",
    "            continue\n",
    "\n",
    "        time_grid = generate_timegrid_eicu(df_v, df_l, step_min)\n",
    "        if len(time_grid) == 0:\n",
    "            continue\n",
    "\n",
    "        df_v_filled = impute_batch_eicu(\n",
    "            df=df_v,\n",
    "            variables=vital_features,\n",
    "            impute_defaults=impute_defaults_vital,\n",
    "            time_grid=time_grid,\n",
    "            offset_col='observationoffset',\n",
    "            stay_id_if_empty=sid\n",
    "        )\n",
    "\n",
    "        df_l_filled = impute_batch_eicu(\n",
    "            df=df_l,\n",
    "            variables=lab_features_lower,\n",
    "            impute_defaults=impute_defaults_lab,\n",
    "            time_grid=time_grid,\n",
    "            offset_col='labresultoffset',\n",
    "            stay_id_if_empty=sid\n",
    "        )\n",
    "\n",
    "        df_merged = pd.merge(df_v_filled, df_l_filled, on=['patientunitstayid','offset_min'], how='outer')\n",
    "\n",
    "        # vasopressor_active (from first infusion onwards)\n",
    "        start = vaso_start.get(sid, None)\n",
    "        if start is None:\n",
    "            df_merged['vasopressor_active'] = 0\n",
    "        else:\n",
    "            df_merged['vasopressor_active'] = (df_merged['offset_min'] >= start).astype(int)\n",
    "\n",
    "        # mortality (ICU discharge)\n",
    "        df_merged['mortality'] = int(mortality_map.get(sid, 0))\n",
    "\n",
    "        # falencia pointwise and windowed (2/3 in 60min)\n",
    "        cond = (df_merged['systemicmean'] < 65)\n",
    "        if 'lactate' in df_merged.columns:\n",
    "            cond = cond | ((df_merged['vasopressor_active'] == 1) & (df_merged['lactate'] >= 2))\n",
    "        df_merged['falencia_point'] = cond.astype(int)\n",
    "\n",
    "        window = int(60/step_min)\n",
    "        threshold = int(np.ceil(window * (2/3)))\n",
    "        df_merged = df_merged.sort_values('offset_min')\n",
    "        df_merged['falencia'] = (\n",
    "            df_merged['falencia_point']\n",
    "            .rolling(window=window, min_periods=window)\n",
    "            .sum()\n",
    "            .ge(threshold)\n",
    "            .astype(int)\n",
    "            .fillna(0)\n",
    "        )\n",
    "\n",
    "        # engineered features - SEPARADAS (1 linha por paciente)\n",
    "        included_vars = []\n",
    "        excluded_vars = []\n",
    "        row = {'patientunitstayid': int(sid)}\n",
    "        for var in vital_features + lab_features_lower:\n",
    "            imp_col = f\"{var}_imputed\"\n",
    "            if imp_col in df_merged.columns:\n",
    "                included_vars.append(var)\n",
    "                n_meas, min_v, max_v, mean_v, instab, intens, cumul = engineered_features(df_merged, var, imp_col)\n",
    "                row[f\"n_meas_{var}\"] = n_meas\n",
    "                row[f\"min_{var}\"] = min_v\n",
    "                row[f\"max_{var}\"] = max_v\n",
    "                row[f\"mean_{var}\"] = mean_v\n",
    "                row[f\"{var}_instab\"] = instab\n",
    "                row[f\"{var}_intens\"] = intens\n",
    "                row[f\"{var}_cumul\"] = cumul\n",
    "            else:\n",
    "                excluded_vars.append(var)\n",
    "\n",
    "        row['mortality'] = int(mortality_map.get(sid, 0))\n",
    "        engineered_rows.append(row)\n",
    "\n",
    "        # store per-patient engineering report\n",
    "        engineered_report.append({\n",
    "            'patientunitstayid': int(sid),\n",
    "            'included': included_vars,\n",
    "            'excluded': excluded_vars,\n",
    "            'n_rows': int(len(df_merged))\n",
    "        })\n",
    "\n",
    "        merged_data.append(df_merged)\n",
    "\n",
    "    if not merged_data:\n",
    "        return pd.DataFrame(), pd.DataFrame(), engineered_report\n",
    "\n",
    "    result = pd.concat(merged_data, ignore_index=True)\n",
    "\n",
    "    # Round numeric columns to 3 decimal places - vectorized\n",
    "    numeric_cols = result.select_dtypes(include=[np.number]).columns\n",
    "    result[numeric_cols] = result[numeric_cols].round(3)\n",
    "\n",
    "    features_df = pd.DataFrame(engineered_rows)\n",
    "    if not features_df.empty:\n",
    "        num_cols = features_df.select_dtypes(include=[np.number]).columns\n",
    "        features_df[num_cols] = features_df[num_cols].round(3)\n",
    "\n",
    "    return result, features_df, engineered_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686c2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar batches de features\n",
    "FEATURE_DIR = Path(OUTPUT_DIR) / 'preprocess/batches/features_filtered'\n",
    "FEATURE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FEATURES_ONLY_DIR = Path(OUTPUT_DIR) / 'preprocess/engineered_features'\n",
    "FEATURES_ONLY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "batch_size = 1000\n",
    "print('Total stays:', len(patient_ids))\n",
    "\n",
    "start_time = time.time()\n",
    "pbar = tqdm(range(0, len(patient_ids), batch_size), desc='Batches', unit='batch')\n",
    "\n",
    "for bidx, i in enumerate(pbar):\n",
    "    batch_ids = patient_ids[i:i+batch_size]\n",
    "\n",
    "    out_path = FEATURE_DIR / f'batch_{bidx:04d}_sub00.parquet'\n",
    "    done_flag = FEATURE_DIR / f'batch_{bidx:04d}_sub00.parquet.done'\n",
    "    report_path = FEATURE_DIR / f'batch_{bidx:04d}_sub00.report.json'\n",
    "\n",
    "    feat_out_path = FEATURES_ONLY_DIR / f'features_{bidx:04d}.parquet'\n",
    "    feat_done_flag = FEATURES_ONLY_DIR / f'features_{bidx:04d}.parquet.done'\n",
    "\n",
    "    if not done_flag.exists() or not feat_done_flag.exists():\n",
    "        df_batch, df_features, report = process_eicu_batch(batch_ids, step_min=5)\n",
    "\n",
    "        if df_batch is None or (hasattr(df_batch, 'empty') and df_batch.empty) or (isinstance(df_batch, pd.DataFrame) and df_batch.shape[0] == 0):\n",
    "            done_flag.write_text('empty')\n",
    "            if report:\n",
    "                report_path.write_text(json.dumps(report, ensure_ascii=False))\n",
    "        else:\n",
    "            df_batch.to_parquet(out_path, index=False)\n",
    "            done_flag.write_text('ok')\n",
    "            if report:\n",
    "                report_path.write_text(json.dumps(report, ensure_ascii=False))\n",
    "\n",
    "        if df_features is None or (hasattr(df_features, 'empty') and df_features.empty) or (isinstance(df_features, pd.DataFrame) and df_features.shape[0] == 0):\n",
    "            feat_done_flag.write_text('empty')\n",
    "        else:\n",
    "            df_features.to_parquet(feat_out_path, index=False)\n",
    "            feat_done_flag.write_text('ok')\n",
    "\n",
    "        # quick progress summary for this batch\n",
    "        if report:\n",
    "            n_pat = len(report)\n",
    "            total_rows = sum(r.get('n_rows', 0) for r in report)\n",
    "            from collections import Counter\n",
    "            exc = Counter()\n",
    "            for r in report:\n",
    "                exc.update(r.get('excluded', []))\n",
    "            most_excluded = exc.most_common(5)\n",
    "            pbar.set_postfix_str(f\"proc={n_pat} stays, rows={total_rows}, top_exc={most_excluded[:3]}\")\n",
    "\n",
    "    if (bidx + 1) % 5 == 0:\n",
    "        elapsed_min = (time.time() - start_time) / 60.0\n",
    "        pbar.set_postfix_str(f\"elapsed={elapsed_min:.1f}m\")\n",
    "\n",
    "elapsed_min = (time.time() - start_time) / 60.0\n",
    "print(f'Done. Features saved in: {FEATURE_DIR} (elapsed {elapsed_min:.1f} min)')\n",
    "print(f'Done. Engineered features saved in: {FEATURES_ONLY_DIR} (elapsed {elapsed_min:.1f} min)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947f47c0",
   "metadata": {},
   "source": [
    "## Build HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f489489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build HDF5 (eICU) with engineered static features\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables\n",
    "\n",
    "BATCH_DIR = Path(OUT_DIR) / \"preprocess\" / \"batches\" / \"features_filtered\"\n",
    "ENGINEERED_DIR = Path(OUT_DIR) / \"preprocess\" / \"engineered_features\"\n",
    "H5_PATH = Path(OUT_DIR) / \"preprocess\" / \"h5\" / \"dataset_eicu.h5\"\n",
    "SPLIT_TSV = Path(OUT_DIR) / \"split_all.tsv\"\n",
    "\n",
    "ID_COL = \"patientunitstayid\"\n",
    "TIME_COL = \"offset_min\"\n",
    "LABEL_COL = \"falencia\"\n",
    "SEED = 42\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC = 0.1\n",
    "\n",
    "# --- helpers ---\n",
    "def ensure_groups(h5, groups):\n",
    "    for g in groups:\n",
    "        if f\"/{g}\" not in h5:\n",
    "            h5.create_group(\"/\", g, f\"{g} group\")\n",
    "\n",
    "\n",
    "def create_earray(h5, path, atom, n_cols, expectedrows=10_000_000):\n",
    "    parent, name = path.rsplit(\"/\", 1)\n",
    "    return h5.create_earray(parent, name, atom=atom, shape=(0, n_cols), expectedrows=expectedrows)\n",
    "\n",
    "\n",
    "def build_split(stay_ids, seed, train_frac, val_frac):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    stay_ids = np.array(sorted(stay_ids), dtype=int)\n",
    "    rng.shuffle(stay_ids)\n",
    "    n = len(stay_ids)\n",
    "    n_train = int(n * train_frac)\n",
    "    n_val = int(n * val_frac)\n",
    "    return {\n",
    "        \"train\": set(stay_ids[:n_train].tolist()),\n",
    "        \"val\": set(stay_ids[n_train:n_train + n_val].tolist()),\n",
    "        \"test\": set(stay_ids[n_train + n_val:].tolist()),\n",
    "    }\n",
    "\n",
    "\n",
    "def load_split_map(split_tsv, stay_ids, seed, train_frac, val_frac):\n",
    "    if split_tsv.exists():\n",
    "        df = pd.read_csv(split_tsv, sep=\"\t\")\n",
    "        if {ID_COL, \"split\"}.issubset(df.columns):\n",
    "            return dict(zip(df[ID_COL].astype(int), df[\"split\"]))\n",
    "        if {\"stay_id\", \"split\"}.issubset(df.columns):\n",
    "            return dict(zip(df[\"stay_id\"].astype(int), df[\"split\"]))\n",
    "    split_sets = build_split(stay_ids, seed, train_frac, val_frac)\n",
    "    split_tsv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with split_tsv.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"stay_id\\tsplit\\n\")\n",
    "        for split, ids in split_sets.items():\n",
    "            for sid in sorted(ids):\n",
    "                f.write(f\"{sid}\\t{split}\\n\")\n",
    "    return {sid: split for split, ids in split_sets.items() for sid in ids}\n",
    "\n",
    "\n",
    "paths = sorted(p for p in BATCH_DIR.glob(\"batch_*.parquet\") if \"_amostra\" not in p.name)\n",
    "if not paths:\n",
    "    raise SystemExit(f\"No batch_*.parquet found in {BATCH_DIR}\")\n",
    "\n",
    "# collect stay ids\n",
    "stay_ids = set()\n",
    "for p in paths:\n",
    "    df_ids = pd.read_parquet(p, columns=[ID_COL])\n",
    "    stay_ids.update(df_ids[ID_COL].dropna().astype(int).unique().tolist())\n",
    "\n",
    "split_map = load_split_map(SPLIT_TSV, stay_ids, SEED, TRAIN_FRAC, VAL_FRAC)\n",
    "\n",
    "# base columns\n",
    "first_df = pd.read_parquet(paths[0])\n",
    "drop_cols = {ID_COL, TIME_COL, \"falencia\", \"falencia_point\", \"mortality\"}\n",
    "base_cols = [c for c in first_df.columns if c not in drop_cols]\n",
    "if LABEL_COL not in first_df.columns:\n",
    "    raise SystemExit(f\"Label column '{LABEL_COL}' not found in {paths[0].name}\")\n",
    "\n",
    "# engineered (static) columns\n",
    "engineered_cols = []\n",
    "feat0 = ENGINEERED_DIR / \"features_0000.parquet\"\n",
    "if feat0.exists():\n",
    "    feat_df = pd.read_parquet(feat0)\n",
    "    engineered_cols = [\n",
    "        c for c in feat_df.columns\n",
    "        if c not in {ID_COL, \"mortality\", \"falencia\", \"falencia_point\"}\n",
    "    ]\n",
    "\n",
    "H5_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "if H5_PATH.exists():\n",
    "    H5_PATH.unlink()\n",
    "\n",
    "with tables.open_file(H5_PATH, mode=\"w\") as h5:\n",
    "    ensure_groups(h5, [\"data\", \"labels\", \"patient_windows\"])\n",
    "    if engineered_cols:\n",
    "        ensure_groups(h5, [\"static\"])\n",
    "\n",
    "    data_arrays = {}\n",
    "    label_arrays = {}\n",
    "    window_arrays = {}\n",
    "    stay_id_arrays = {}\n",
    "    static_arrays = {}\n",
    "\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        data_arrays[split] = create_earray(h5, f\"/data/{split}\", tables.Float32Atom(), n_cols=len(base_cols))\n",
    "        label_arrays[split] = create_earray(h5, f\"/labels/{split}\", tables.Float32Atom(), n_cols=1)\n",
    "        window_arrays[split] = create_earray(h5, f\"/patient_windows/{split}\", tables.Int32Atom(), n_cols=3)\n",
    "        stay_id_arrays[split] = h5.create_earray(\"/patient_windows\", f\"{split}_stay_ids\", atom=tables.Int32Atom(), shape=(0,))\n",
    "        if engineered_cols:\n",
    "            static_arrays[split] = create_earray(h5, f\"/static/{split}\", tables.Float32Atom(), n_cols=len(engineered_cols))\n",
    "\n",
    "    for idx, path in enumerate(paths):\n",
    "        print(f\"Reading {path}\")\n",
    "        df = pd.read_parquet(path)\n",
    "        df[ID_COL] = df[ID_COL].astype(int)\n",
    "        if TIME_COL in df.columns:\n",
    "            df = df.sort_values([ID_COL, TIME_COL])\n",
    "\n",
    "        engineered_map = {}\n",
    "        if engineered_cols:\n",
    "            feat_path = ENGINEERED_DIR / f\"features_{idx:04d}.parquet\"\n",
    "            if feat_path.exists():\n",
    "                feat_df = pd.read_parquet(feat_path)\n",
    "                feat_df[ID_COL] = feat_df[ID_COL].astype(int)\n",
    "                feat_df = feat_df.set_index(ID_COL)[engineered_cols]\n",
    "                engineered_map = {int(sid): row.to_numpy(dtype=np.float32, copy=False) for sid, row in feat_df.iterrows()}\n",
    "            else:\n",
    "                print(f\"WARNING: missing engineered file {feat_path}, will fill NaNs\")\n",
    "\n",
    "        for stay_id, df_sid in df.groupby(ID_COL):\n",
    "            split = split_map.get(int(stay_id), \"train\")\n",
    "            d_arr = data_arrays[split]\n",
    "            l_arr = label_arrays[split]\n",
    "            w_arr = window_arrays[split]\n",
    "            sid_arr = stay_id_arrays[split]\n",
    "\n",
    "            start = d_arr.nrows\n",
    "            base_feat = df_sid[base_cols].astype(np.float32).to_numpy()\n",
    "            lbl = df_sid[[LABEL_COL]].astype(np.float32).to_numpy()\n",
    "            d_arr.append(base_feat)\n",
    "            l_arr.append(lbl)\n",
    "            stop = d_arr.nrows\n",
    "\n",
    "            w_arr.append(np.array([[start, stop, int(stay_id)]], dtype=np.int32))\n",
    "            sid_arr.append(np.array([int(stay_id)], dtype=np.int32))\n",
    "\n",
    "            if engineered_cols:\n",
    "                static = engineered_map.get(int(stay_id))\n",
    "                if static is None:\n",
    "                    static_vals = np.full((1, len(engineered_cols)), np.nan, dtype=np.float32)\n",
    "                else:\n",
    "                    static_vals = static[None, :]\n",
    "                static_arrays[split].append(static_vals.astype(np.float32, copy=False))\n",
    "\n",
    "    h5.set_node_attr(\"/\", \"feature_names\", base_cols)\n",
    "    if engineered_cols:\n",
    "        h5.set_node_attr(\"/\", \"static_feature_names\", engineered_cols)\n",
    "    h5.set_node_attr(\"/\", \"label_column\", LABEL_COL)\n",
    "\n",
    "print(\"H5 saved to\", H5_PATH, \"with\", len(base_cols), \"dynamic features and\", len(engineered_cols), \"static features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfbf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build per-stay falencia summary (falencia_normal + 45/60min + mortality)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "PREPROCESS_DIR = Path(OUTPUT_DIR) / \"preprocess\"\n",
    "INPUT_DIR = PREPROCESS_DIR / \"batches\" / \"features_filtered\"\n",
    "OUT_CSV = PREPROCESS_DIR / \"falencia_stay_summary.csv\"\n",
    "MORTALITY_PATH = PREPROCESS_DIR / \"mortality_by_stay.csv\"\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    str(Path(\"src/clustering/minirocket/pipelines/build_falencia_stay_summary.py\")),\n",
    "    \"--input_dir\", str(INPUT_DIR),\n",
    "    \"--mortality_path\", str(MORTALITY_PATH),\n",
    "    \"--out_csv\", str(OUT_CSV),\n",
    "    \"--time_col\", \"offset_min\",\n",
    "    \"--id_col\", \"patientunitstayid\",\n",
    "    \"--mbp_col\", \"systemicmean\",\n",
    "    \"--vaso_col\", \"vasopressor_ativo\",\n",
    "    \"--lactate_col\", \"lactate\",\n",
    "    \"--falencia_col\", \"falencia\",\n",
    "    \"--step_min_45\", \"5\", \"--window_min_45\", \"45\",\n",
    "    \"--step_min_60\", \"60\", \"--window_min_60\", \"60\",\n",
    "]\n",
    "print(\" \".join(cmd))\n",
    "subprocess.run(cmd, check=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
