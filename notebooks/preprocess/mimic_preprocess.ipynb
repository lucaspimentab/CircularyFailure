{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04bb39f9",
   "metadata": {},
   "source": [
    "# MIMIC Preprocess (standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a855a5",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33bc22b",
   "metadata": {},
   "source": [
    "### 0.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import tables\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df97255",
   "metadata": {},
   "source": [
    "### 0.2 Paths & environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Caminhos principais\n",
    "DATA_DIR = Path(r\"<DATA_DIR>/mimic\")\n",
    "OUT_DIR = Path(r\"<OUTPUT_DIR>/mimic\")\n",
    "OUTPUT_ROOT = str(OUT_DIR)  # usado nos paths do notebook\n",
    "\n",
    "PREPROCESS_DIR = OUT_DIR / \"preprocess\"\n",
    "BATCHES_DIR = PREPROCESS_DIR / \"batches\"\n",
    "H5_DIR = PREPROCESS_DIR / \"h5\"\n",
    "REPORTS_DIR = OUT_DIR / \"reports\"\n",
    "\n",
    "PREPROCESS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BATCHES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "H5_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Conexao MIMIC-IV (Postgres) via variaveis de ambiente\n",
    "DB_NAME = os.getenv(\"MIMIC_DB_NAME\", \"mimiciv\")\n",
    "DB_HOST = os.getenv(\"MIMIC_DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"MIMIC_DB_PORT\", \"5432\")\n",
    "DB_USER = os.getenv(\"MIMIC_DB_USER\", \"\")\n",
    "DB_PASSWORD = os.getenv(\"MIMIC_DB_PASSWORD\", \"\")\n",
    "\n",
    "if not DB_USER or not DB_PASSWORD:\n",
    "    print(\"[WARN] MIMIC_DB_USER/MIMIC_DB_PASSWORD nao definidos. Configure no .env ou no ambiente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ede60",
   "metadata": {},
   "source": [
    "### 0.3 Defaults / mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of clinical variables grouped by sampling frequency.\n",
    "# These groups help in designing the time grid and imputation strategy later on.\n",
    "\n",
    "# High-frequency variables: typically measured every few minutes to 1 hour\n",
    "vars_high_freq = [\n",
    "    'heart_rate', 'sbp', 'dbp', 'mbp', 'resp_rate', 'spo2',\n",
    "    'lab_53085', 'lab_51580', 'lab_52642', 'lab_51002', 'lab_52116', 'lab_51623',\n",
    "    'lab_50928', 'lab_52117', 'lab_50855', 'lab_52546', 'lab_53161', 'lab_53180',\n",
    "    'lab_52142', 'lab_51266', 'lab_52144', 'lab_51631', 'lab_51638', 'lab_51640',\n",
    "    'lab_51647', 'lab_51643', 'lab_50975', 'lab_51292', 'lab_51290', 'lab_51291',\n",
    "    'lab_52551', 'lab_51568', 'lab_51569', 'lab_51570', 'lab_51464', 'lab_51966'\n",
    "]\n",
    "\n",
    "# Medium-frequency variables: measured every few hours\n",
    "vars_medium_freq = [\n",
    "    'temperature', 'glucose', 'lab_50908', 'lab_50915', 'lab_50856', 'lab_50803',\n",
    "    'lab_50805', 'lab_50808', 'lab_50809', 'lab_50813'\n",
    "]\n",
    "\n",
    "# Low-frequency variables: measured once or twice per day\n",
    "vars_low_freq = [\n",
    "    'lab_50861', 'lab_50862', 'lab_50883', 'lab_50884', 'lab_50885', 'lab_50910',\n",
    "    'lab_50924', 'lab_50963', 'lab_51003', 'lab_50889', 'lab_51214', 'lab_50878',\n",
    "    'lab_50912', 'lab_51265', 'lab_50931', 'lab_50935', 'lab_51222', 'lab_51223',\n",
    "    'lab_50852', 'lab_50971', 'lab_50983', 'lab_50990', 'lab_50967', 'lab_50968',\n",
    "    'lab_50969', 'lab_50960', 'lab_50966', 'lab_50970', 'lab_51099', 'lab_51006',\n",
    "    'lab_51274', 'lab_51275', 'lab_51196'\n",
    "]\n",
    "\n",
    "# Vasopressors: medication administration indicators\n",
    "vasopressor_vars = [\n",
    "    'dopamine', 'epinephrine', 'norepinephrine', 'phenylephrine',\n",
    "    'vasopressin', 'dobutamine', 'milrinone'\n",
    "]\n",
    "\n",
    "# Summary\n",
    "print(f\"High-frequency variables: {len(vars_high_freq)}\")\n",
    "print(f\"Medium-frequency variables: {len(vars_medium_freq)}\")\n",
    "print(f\"Low-frequency variables: {len(vars_low_freq)}\")\n",
    "print(f\"Vasopressors: {len(vasopressor_vars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values (NORMAL_VALUES) are used to initialize or impute missing values\n",
    "# for key vital signs and routine measurements, following standard reference values\n",
    "# from the original paper and clinical guidelines.\n",
    "\n",
    "NORMAL_VALUES = {\n",
    "    'heart_rate': 70,\n",
    "    'sbp': 125,\n",
    "    'dbp': 75,\n",
    "    'mbp': 90,\n",
    "    'resp_rate': 12,\n",
    "    'temperature': 37,\n",
    "    'spo2': 98,\n",
    "    'glucose': 5,\n",
    "}\n",
    "\n",
    "# PARAMETROS_IMPUTACAO contains default values and acceptable clinical ranges\n",
    "# for laboratory variables. These ranges are used to filter outliers and apply\n",
    "# median/IQR-based imputation when appropriate.\n",
    "\n",
    "PARAMETROS_IMPUTACAO = {\n",
    "    # Format: variable: {'default': value, 'range': (min, max)}\n",
    "    'lab_50803': {'default': 24, 'range': (22, 26)},                   # Calculated Bicarbonate, Whole Blood (mEq/L)\n",
    "    'lab_50805': {'default': 1.5, 'range': (0.5, 2.0)},                # Carboxyhemoglobin, Blood (%)\n",
    "    'lab_50808': {'default': 1.1, 'range': (0.9, 1.3)},                # Free Calcium, Blood (mmol/L)\n",
    "    'lab_50809': {'default': 90, 'range': (70, 110)},                  # Glucose, Blood (mg/dL)\n",
    "    'lab_50813': {'default': 1.0, 'range': (0.5, 2.2)},                # Lactate, Blood, Blood Gas (mmol/L)\n",
    "    'lab_50852': {'default': 5.5, 'range': (4.0, 6.5)},                # % Hemoglobin A1c, Blood (%)\n",
    "    'lab_50855': {'default': 14, 'range': (13, 17)},                   # Absolute Hemoglobin, Blood (g/dL)\n",
    "    'lab_50856': {'default': 10, 'range': (5, 20)},                    # Acetaminophen, Blood (mg/L)\n",
    "    'lab_50861': {'default': 30, 'range': (10, 50)},                   # Alanine Aminotransferase (ALT), Blood (U/L)\n",
    "    'lab_50862': {'default': 4.3, 'range': (3.5, 5.5)},                # Albumin, Blood (g/dL)\n",
    "    'lab_50878': {'default': 40, 'range': (10, 40)},                   # Asparate Aminotransferase (AST), Blood (U/L)\n",
    "    'lab_50883': {'default': 0.3, 'range': (0.1, 0.4)},                # Bilirubin, Direct, Blood (mg/dL)\n",
    "    'lab_50884': {'default': 0.6, 'range': (0.3, 1.2)},                # Bilirubin, Indirect, Blood (mg/dL)\n",
    "    'lab_50885': {'default': 1.0, 'range': (0.3, 1.9)},                # Bilirubin, Total, Blood (mg/dL)\n",
    "    'lab_50889': {'default': 5.0, 'range': (3.5, 5.1)},                # C-Reactive Protein, Blood (mg/dL)\n",
    "    'lab_50908': {'default': 0.8, 'range': (0.7, 1.3)},                # CK-MB Index, Blood (ng/mL)\n",
    "    'lab_50910': {'default': 150, 'range': (50, 200)},                 # Creatine Kinase (CK), Blood (U/L)\n",
    "    'lab_50912': {'default': 1.0, 'range': (0.6, 1.3)},                # Creatinine, Blood (mg/dL)\n",
    "    'lab_50915': {'default': 0.5, 'range': (0.2, 0.5)},                # D-Dimer, Blood (µg/mL)\n",
    "    'lab_50924': {'default': 120, 'range': (20, 300)},                 # Ferritin, Blood (ng/mL)\n",
    "    'lab_50928': {'default': 100, 'range': (0, 150)},                  # Gastrin, Blood (pg/mL)\n",
    "    'lab_50931': {'default': 90, 'range': (70, 110)},                  # Glucose, Blood (mg/dL)\n",
    "    'lab_50935': {'default': 100, 'range': (30, 200)},                 # Haptoglobin, Blood (mg/dL)\n",
    "    'lab_50960': {'default': 2.0, 'range': (1.5, 2.5)},                # Magnesium, Blood (mg/dL)\n",
    "    'lab_50963': {'default': 100, 'range': (20, 400)},                 # NTproBNP, Blood (pg/mL)\n",
    "    'lab_50966': {'default': 15, 'range': (5, 20)},                    # Phenobarbital, Blood (µg/mL)\n",
    "    'lab_50967': {'default': 15, 'range': (5, 20)},                    # Phenytoin, Blood (µg/mL)\n",
    "    'lab_50968': {'default': 10, 'range': (5, 15)},                    # Phenytoin, Free, Blood (µg/mL)\n",
    "    'lab_50969': {'default': 80, 'range': (70, 90)},                   # Phenytoin, Percent Free (%)\n",
    "    'lab_50970': {'default': 3.5, 'range': (2.5, 4.5)},                # Phosphate, Blood (mg/dL)\n",
    "    'lab_50971': {'default': 4.5, 'range': (3.5, 5.1)},                # Potassium, Blood (mmol/L)\n",
    "    'lab_50975': {'default': 5.5, 'range': (4.5, 6.5)},                # Protein Electrophoresis, Blood (g/dL)\n",
    "    'lab_50983': {'default': 140, 'range': (135, 145)},                # Sodium, Blood (mmol/L)\n",
    "    'lab_50990': {'default': 10, 'range': (5, 20)},                    # Theophylline, Blood (µg/mL)\n",
    "    'lab_51002': {'default': 0.01, 'range': (0, 0.1)},                 # Troponin I, Blood (ng/mL)\n",
    "    'lab_51003': {'default': 0.01, 'range': (0, 0.1)},                 # Troponin T, Blood (ng/mL)\n",
    "    'lab_51006': {'default': 15, 'range': (5, 20)},                    # Urea Nitrogen, Blood (mg/dL)\n",
    "    'lab_51099': {'default': 0.2, 'range': (0, 0.5)},                  # Protein/Creatinine Ratio, Urine\n",
    "    'lab_51196': {'default': 0.5, 'range': (0, 1)},                    # D-Dimer, Blood (mg/mL)\n",
    "    'lab_51214': {'default': 300, 'range': (200, 400)},                # Fibrinogen, Functional, Blood (mg/dL)\n",
    "    'lab_51222': {'default': 14, 'range': (12, 16)},                   # Hemoglobin, Blood (g/dL)\n",
    "    'lab_51223': {'default': 3.0, 'range': (2.5, 3.5)},                # Hemoglobin A2, Blood (%)\n",
    "    'lab_51265': {'default': 200, 'range': (150, 300)},                # Platelet Count, Blood (x10^3/uL)\n",
    "    'lab_51266': {'default': 1.0, 'range': (0.5, 1.5)},                # Platelet Smear, Blood (%)\n",
    "    'lab_51274': {'default': 12, 'range': (10, 14)},                   # PT, Blood (seconds)\n",
    "    'lab_51275': {'default': 35, 'range': (30, 40)},                   # PTT, Blood (seconds)\n",
    "    'lab_51290': {'default': 0, 'range': (0, 1)},                      # Sickle Cell Preparation, Blood (binary)\n",
    "    'lab_51291': {'default': 0, 'range': (0, 1)},                      # Sickle Cells, Blood (binary)\n",
    "    'lab_51292': {'default': 0, 'range': (0, 1)},                      # Spherocytes, Blood (binary)\n",
    "    'lab_51464': {'default': 0, 'range': (0, 1)},                      # Bilirubin, Urine (binary)\n",
    "    'lab_51568': {'default': 0.2, 'range': (0, 1)},                    # Bilirubin, Neonatal, Blood (mg/dL)\n",
    "    'lab_51569': {'default': 0.2, 'range': (0, 1)},                    # Bilirubin, Neonatal, Direct, Blood (mg/dL)\n",
    "    'lab_51570': {'default': 0.2, 'range': (0, 1)},                    # Bilirubin, Neonatal, Indirect, Blood (mg/dL)\n",
    "    'lab_51580': {'default': 0.8, 'range': (0.5, 1.5)},                # Calculated CK-MB, Blood (ng/mL)\n",
    "    'lab_51623': {'default': 300, 'range': (200, 400)},                # Fibrinogen, Blood (mg/dL)\n",
    "    'lab_51631': {'default': 6.0, 'range': (4.5, 7.0)},                # Glycated Hemoglobin, Blood (%)\n",
    "    'lab_51638': {'default': 45, 'range': (10, 50)},                   # Hematocrit, Blood (%)\n",
    "    'lab_51640': {'default': 14, 'range': (12, 16)},                   # Hemoglobin, Blood (g/dL)\n",
    "    'lab_51643': {'default': 3.0, 'range': (2.5, 3.5)},                # Hemoglobin A2, Blood (%)\n",
    "    'lab_51647': {'default': 3.0, 'range': (2.5, 3.5)},                # Hemoglobin S, Blood (%)\n",
    "    'lab_51966': {'default': 0, 'range': (0, 1)},                      # Bilirubin, Urine (binary)\n",
    "    'lab_52116': {'default': 300, 'range': (200, 400)},                # Fibrinogen, Blood (mg/dL)\n",
    "    'lab_52117': {'default': 300, 'range': (200, 400)},                # Fibrinogen, Immunologic, Blood (mg/dL)\n",
    "    'lab_52142': {'default': 10, 'range': (7, 13)},                    # Mean Platelet Volume, Blood (fL)\n",
    "    'lab_52144': {'default': 1.0, 'range': (0.5, 1.5)},                # Methemoglobin, Blood (%)\n",
    "    'lab_52546': {'default': 1.0, 'range': (0.5, 1.5)},                # Creatinine, Blood (mg/dL)\n",
    "    'lab_52551': {'default': 0.5, 'range': (0.2, 1.0)},                # D-Dimer, Blood (mg/mL)\n",
    "    'lab_52642': {'default': 0.01, 'range': (0, 0.1)},                 # Troponin I, Blood (ng/mL)\n",
    "    'lab_53085': {'default': 4.3, 'range': (3.5, 5.5)},                # Albumin, Blood (g/dL)\n",
    "}\n",
    "\n",
    "print(\"Normal values and imputation parameters loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts and pivots laboratory test results for a batch of ICU stays.\n",
    "# Lab values are reshaped into a wide format: one row per (stay_id, charttime) with columns named lab_<itemid>.\n",
    "\n",
    "# Create a dictionary of default values (to be used in imputation later)\n",
    "default_lab_values = {k: v['default'] for k, v in PARAMETROS_IMPUTACAO.items()}\n",
    "\n",
    "# List of itemids to extract from labevents\n",
    "lab_itemids = [\n",
    "    50861, 50862, 53085, 50908, 51580, 50883, 50884, 50885, 50910, 50924,\n",
    "    50963, 52642, 51002, 51003, 50889, 52116, 51623, 50928, 52117,\n",
    "    51214, 50878, 50855, 50912, 52546, 53161, 53180, 52142, 51265, 51266,\n",
    "    52144, 50931, 50935, 51631, 51638, 51640, 51222, 51223, 50856, 51647,\n",
    "    50852, 51643, 50971, 50983, 50990, 50967, 50968, 50969, 50960, 50966,\n",
    "    50970, 50975, 51099, 51006, 51274, 51275, 51292, 51290, 51291,\n",
    "    51196, 52551, 50915, 51568, 51569, 51570, 51464, 51966, 50803, 50805,\n",
    "    50808, 50809, 50813\n",
    "]\n",
    "\n",
    "def query_labs_batch(batch_stay_ids):\n",
    "    stay_id_str = \",\".join(map(str, batch_stay_ids))\n",
    "    itemid_str = \",\".join(map(str, lab_itemids))\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        icu.stay_id,\n",
    "        le.charttime,\n",
    "        le.itemid,\n",
    "        le.valuenum\n",
    "    FROM mimiciv_hosp.labevents le\n",
    "    JOIN mimiciv_icu.icustays icu \n",
    "        ON le.subject_id = icu.subject_id AND le.hadm_id = icu.hadm_id\n",
    "    WHERE le.itemid IN ({itemid_str})\n",
    "      AND le.valuenum IS NOT NULL\n",
    "      AND icu.stay_id IN (SELECT stay_id FROM todas_utis)\n",
    "      AND icu.stay_id IN ({stay_id_str})\n",
    "    ORDER BY icu.stay_id, le.charttime;\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Querying lab results for {len(batch_stay_ids)} stay_ids...\")\n",
    "    df_labs = pd.read_sql(query, conn)\n",
    "    df_labs['charttime'] = pd.to_datetime(df_labs['charttime'])\n",
    "    print(f\"Query completed. Retrieved {len(df_labs)} rows.\")\n",
    "\n",
    "    # Pivot to wide format: one row per stay_id x charttime\n",
    "    df_wide = df_labs.pivot_table(index=['stay_id', 'charttime'], columns='itemid', values='valuenum')\n",
    "    df_wide.columns = [f'lab_{col}' for col in df_wide.columns]\n",
    "    df_wide = df_wide.reset_index().sort_values(['stay_id', 'charttime']).reset_index(drop=True)\n",
    "\n",
    "    return df_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d924891",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR_FINAL = f\"{OUTPUT_ROOT}/preprocess/batches/merged\"\n",
    "os.makedirs(OUTPUT_DIR_FINAL, exist_ok=True)\n",
    "\n",
    "\n",
    "def is_batch_processed(batch_idx, output_dir):\n",
    "    \"\"\"Check if a batch has already been processed by looking for a .done flag.\"\"\"\n",
    "    return os.path.exists(os.path.join(output_dir, f'batch_{batch_idx:04d}.parquet.done'))\n",
    "\n",
    "\n",
    "def save_batch(df, batch_idx, output_dir):\n",
    "    \"\"\"Save processed batch to .parquet and create a .done flag.\"\"\"\n",
    "    parquet_path = os.path.join(output_dir, f'batch_{batch_idx:04d}.parquet')\n",
    "    done_path = os.path.join(output_dir, f'batch_{batch_idx:04d}.parquet.done')\n",
    "\n",
    "    df.to_parquet(parquet_path)\n",
    "    with open(done_path, 'w') as f:\n",
    "        f.write('done')\n",
    "    print(f\"Batch {batch_idx} saved.\")\n",
    "\n",
    "\n",
    "def extract_stays_from_csv(path):\n",
    "    \"\"\"Load a list of stay_ids from CSV file with a 'stay_id' column.\"\"\"\n",
    "    return pd.read_csv(path)['stay_id'].unique().tolist()\n",
    "\n",
    "\n",
    "# Load pre-extracted stay_ids from vitals and labs\n",
    "stays_labs = extract_stays_from_csv(f\"{OUTPUT_ROOT}/preprocess/stays_labs.csv\")\n",
    "stays_vitals = extract_stays_from_csv(f\"{OUTPUT_ROOT}/preprocess/stays_vitals.csv\")\n",
    "\n",
    "# Use only stays that exist in both datasets\n",
    "stay_ids_common = sorted(set(stays_labs).intersection(stays_vitals))\n",
    "\n",
    "# Define batches of fixed size\n",
    "BATCH_SIZE = 5000\n",
    "batches = [stay_ids_common[i:i + BATCH_SIZE] for i in range(0, len(stay_ids_common), BATCH_SIZE)]\n",
    "\n",
    "print(f\"Total common stays: {len(stay_ids_common)}\")\n",
    "print(f\"Total batches: {len(batches)}\")\n",
    "\n",
    "# Define variable names for vitals and labs\n",
    "vital_vars = ['heart_rate', 'sbp', 'dbp', 'mbp', 'resp_rate', 'temperature', 'spo2', 'glucose']\n",
    "lab_vars = [f'lab_{i}' for i in lab_itemids]\n",
    "\n",
    "\n",
    "# Main batch loop\n",
    "for batch_idx, stay_ids in enumerate(batches):\n",
    "    if is_batch_processed(batch_idx, OUTPUT_DIR_FINAL):\n",
    "        print(f\"Skipping batch {batch_idx} (already processed).\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing batch {batch_idx} with {len(stay_ids)} stays...\")\n",
    "\n",
    "    df_vitals = query_vitals_batch(stay_ids, conn)\n",
    "    df_labs = query_labs_batch(stay_ids)\n",
    "\n",
    "    merged_data = []\n",
    "\n",
    "    for sid in stay_ids:\n",
    "        df_v = df_vitals[df_vitals['stay_id'] == sid]\n",
    "        df_l = df_labs[df_labs['stay_id'] == sid]\n",
    "\n",
    "        if df_v.empty and df_l.empty:\n",
    "            continue\n",
    "\n",
    "        timegrid = generate_timegrid(df_v, df_l)\n",
    "        if timegrid.empty:\n",
    "            continue\n",
    "\n",
    "        timegrid_dt = pd.to_datetime(timegrid, unit='s')\n",
    "\n",
    "        df_v_filled = impute_batch_with_grid(df_v, vital_vars, NORMAL_VALUES, timegrid, timegrid_dt, sid)\n",
    "        df_l_filled = impute_batch_with_grid(df_l, lab_vars, default_lab_values, timegrid, timegrid_dt, sid)\n",
    "\n",
    "        df_merged = pd.merge(df_v_filled, df_l_filled, on=['stay_id', 'charttime'], how='outer')\n",
    "        merged_data.append(df_merged)\n",
    "\n",
    "    if merged_data:\n",
    "        df_batch = pd.concat(merged_data, ignore_index=True)\n",
    "        save_batch(df_batch, batch_idx, OUTPUT_DIR_FINAL)\n",
    "\n",
    "        # Save preview CSV\n",
    "        if len(df_batch) >= 1000:\n",
    "            csv_path = os.path.join(OUTPUT_DIR_FINAL, f'batch_{batch_idx:04d}_sample.csv')\n",
    "            df_batch.head(1000).to_csv(csv_path, index=False)\n",
    "    else:\n",
    "        print(f\"No data in batch {batch_idx}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272846ea",
   "metadata": {},
   "source": [
    "## 1) Preprocess pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7d07e",
   "metadata": {},
   "source": [
    "# MIMIC preprocess (full, organized)\n",
    "\n",
    "Este notebook executa TODO o preprocess do MIMIC (queries + batches + HDF5) em um unico fluxo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888933e5",
   "metadata": {},
   "source": [
    "## 0) Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8535b",
   "metadata": {},
   "source": [
    "### 0.1 Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927254d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Database connection\n",
    "import psycopg2\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# File and model persistence\n",
    "import joblib\n",
    "import tables\n",
    "\n",
    "# Progress bar for loops\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0032c63",
   "metadata": {},
   "source": [
    "### 0.2 Paths & environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3223be31",
   "metadata": {},
   "source": [
    "### 0.3 Variable groups (sampling frequency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a654c",
   "metadata": {},
   "source": [
    "### 0.4 Default values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e193006c",
   "metadata": {},
   "source": [
    "## 1) Preprocess SQL + batches (legacy gitbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714496a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection and ICU admissions extraction\n",
    "# This block connects to a local PostgreSQL database with MIMIC-IV\n",
    "# and creates a temporary table (all_icu_stays) containing ICU admissions longer than 1 hour.\n",
    "\n",
    "# Database connection parameters\n",
    "DB_NAME = os.getenv(\"MIMIC_DB_NAME\", \"mimiciv\")\n",
    "DB_HOST = os.getenv(\"MIMIC_DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"MIMIC_DB_PORT\", \"5432\")\n",
    "DB_USER = os.getenv(\"MIMIC_DB_USER\", \"\")\n",
    "DB_PASSWORD = os.getenv(\"MIMIC_DB_PASSWORD\", \"\")\n",
    "\n",
    "# Attempt to connect to the database\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD,\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT\n",
    "    )\n",
    "    print(\"Connected to the database.\")\n",
    "except Exception as e:\n",
    "    print(\"Connection failed:\", e)\n",
    "\n",
    "# Create a temporary table with ICU stays longer than 1 hour\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute(\"ROLLBACK\")  # Cancel any existing transaction\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TEMP TABLE all_icu_stays AS\n",
    "        SELECT \n",
    "            i.subject_id,\n",
    "            i.hadm_id,\n",
    "            i.stay_id,\n",
    "            i.intime,\n",
    "            i.outtime,\n",
    "            EXTRACT(EPOCH FROM (i.outtime - i.intime)) / 60 AS duration_minutes\n",
    "        FROM mimiciv_icu.icustays i\n",
    "        WHERE EXTRACT(EPOCH FROM (i.outtime - i.intime)) > 3600;\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "\n",
    "# Confirm number of ICU admissions retrieved\n",
    "df_total = pd.read_sql(\"SELECT COUNT(*) AS total_admissions FROM all_icu_stays;\", conn)\n",
    "print(f\"Total ICU admissions longer than 1 hour: {df_total['total_admissions'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unified time grid (5-minute intervals) covering both vitals and labs\n",
    "def generate_timegrid(df_vitals, df_labs):\n",
    "    t0_vitals = df_vitals['charttime'].min() if not df_vitals.empty else None\n",
    "    t0_labs = df_labs['charttime'].min() if not df_labs.empty else None\n",
    "    tmax_vitals = df_vitals['charttime'].max() if not df_vitals.empty else None\n",
    "    tmax_labs = df_labs['charttime'].max() if not df_labs.empty else None\n",
    "\n",
    "    if t0_vitals is None and t0_labs is None:\n",
    "        return pd.DatetimeIndex([])\n",
    "\n",
    "    t0 = min(filter(None, [t0_vitals, t0_labs])).floor('5T')\n",
    "    tmax = max(filter(None, [tmax_vitals, tmax_labs])).ceil('5T')\n",
    "\n",
    "    return pd.date_range(start=t0, end=tmax, freq='5T')\n",
    "\n",
    "\n",
    "# Returns a vector filled with a default value\n",
    "def value_filled_array(size, default_val, dtype=None):\n",
    "    arr = np.empty(size, dtype=dtype or float)\n",
    "    arr[:] = default_val\n",
    "    return arr\n",
    "\n",
    "# Returns an array filled with NaNs\n",
    "def empty_nan_array(size):\n",
    "    arr = np.empty(size)\n",
    "    arr[:] = np.nan\n",
    "    return arr\n",
    "\n",
    "\n",
    "# Check if a batch has already been processed by the existence of a .done file\n",
    "def is_batch_processed(batch_idx, output_dir):\n",
    "    done_flag = os.path.join(output_dir, f'batch_{batch_idx:04d}.parquet.done')\n",
    "    return os.path.exists(done_flag)\n",
    "\n",
    "\n",
    "# Simple forward-fill imputation over a fixed time grid\n",
    "def impute_forward_fill_simple(obs_times, obs_values, time_grid, global_fill=np.nan):\n",
    "    n = len(time_grid)\n",
    "    output = np.full(n, global_fill, dtype=np.float32)\n",
    "    last_val = np.nan\n",
    "    i_obs = 0\n",
    "\n",
    "    for i_pred, t_pred in enumerate(time_grid):\n",
    "        while i_obs < len(obs_times) and obs_times[i_obs] <= t_pred:\n",
    "            if not np.isnan(obs_values[i_obs]):\n",
    "                last_val = obs_values[i_obs]\n",
    "            i_obs += 1\n",
    "        output[i_pred] = last_val if not np.isnan(last_val) else global_fill\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Applies imputation to a batch using the generated time grid\n",
    "def impute_batch_with_grid(df_batch, variables, impute_defaults, time_grid, time_grid_dt, stay_id_if_empty=None):\n",
    "    if df_batch.empty:\n",
    "        if stay_id_if_empty is None:\n",
    "            raise ValueError(\"Empty batch and no fallback stay_id provided.\")\n",
    "        result = {\n",
    "            'stay_id': [stay_id_if_empty] * len(time_grid),\n",
    "            'charttime': time_grid_dt\n",
    "        }\n",
    "        for var in variables:\n",
    "            fill_val = impute_defaults.get(var, np.nan)\n",
    "            result[var] = np.full(len(time_grid), fill_val, dtype=np.float32)\n",
    "            result[f'{var}_imputed'] = np.ones(len(time_grid), dtype=int)\n",
    "        return pd.DataFrame(result)\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for stay_id, group in df_batch.groupby('stay_id'):\n",
    "        group = group.sort_values('charttime').reset_index(drop=True)\n",
    "        timestamps = group['charttime'].values.astype('datetime64[s]').astype(np.int64)\n",
    "\n",
    "        result = {\n",
    "            'stay_id': [stay_id] * len(time_grid),\n",
    "            'charttime': time_grid_dt\n",
    "        }\n",
    "\n",
    "        for var in variables:\n",
    "            if var not in group.columns or group[var].dropna().empty:\n",
    "                fill_val = impute_defaults.get(var, np.nan)\n",
    "                result[var] = np.full(len(time_grid), fill_val, dtype=np.float32)\n",
    "                result[f'{var}_imputed'] = np.ones(len(time_grid), dtype=int)\n",
    "                continue\n",
    "\n",
    "            raw_vals = group[var].values\n",
    "            pred_vals = impute_forward_fill_simple(timestamps, raw_vals, time_grid, impute_defaults.get(var, np.nan))\n",
    "            result[var] = pred_vals\n",
    "\n",
    "            mask = np.zeros(len(time_grid), dtype=int)\n",
    "            valid_ts = timestamps[(timestamps >= time_grid[0]) & (timestamps <= time_grid[-1]) & ~np.isnan(raw_vals)]\n",
    "            idxs = np.searchsorted(time_grid, valid_ts)\n",
    "            idxs = idxs[idxs < len(mask)]\n",
    "            mask[idxs] = 1\n",
    "            result[f'{var}_imputed'] = 1 - mask\n",
    "\n",
    "        frames.append(pd.DataFrame(result))\n",
    "\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "# Query vitals for a batch of stay_ids from MIMIC-IV derived table\n",
    "def query_vitals_batch(batch_stay_ids, conn):\n",
    "    stay_id_list = \",\".join(map(str, batch_stay_ids))\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        vs.stay_id,\n",
    "        vs.charttime,\n",
    "        vs.heart_rate,\n",
    "        vs.sbp,\n",
    "        vs.dbp,\n",
    "        vs.mbp,\n",
    "        vs.resp_rate,\n",
    "        vs.temperature,\n",
    "        vs.spo2,\n",
    "        vs.glucose\n",
    "    FROM mimiciv_derived.vitalsign vs\n",
    "    WHERE vs.stay_id IN ({stay_id_list})\n",
    "    ORDER BY vs.stay_id, vs.charttime;\n",
    "    \"\"\"\n",
    "    print(f\"Querying vitals for {len(batch_stay_ids)} stay_ids...\")\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df['charttime'] = pd.to_datetime(df['charttime'])\n",
    "    print(f\"Query completed. Retrieved {len(df)} rows.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c17da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to input and output directories\n",
    "OUTPUT_DIR_MERGED = f\"{OUTPUT_ROOT}/preprocess/batches/merged\"\n",
    "OUTPUT_DIR_FAILURE = f\"{OUTPUT_ROOT}/preprocess/batches/falencia\"\n",
    "os.makedirs(OUTPUT_DIR_FAILURE, exist_ok=True)\n",
    "\n",
    "# List of vasopressors used for circulatory failure definition\n",
    "vasopressor_cols = [\n",
    "    'dopamine', 'epinephrine', 'norepinephrine', 'phenylephrine',\n",
    "    'vasopressin', 'dobutamine', 'milrinone'\n",
    "]\n",
    "\n",
    "def process_batch_if_needed(batch_idx):\n",
    "    \"\"\"\n",
    "    Skips processing if this batch has already been labeled and saved.\n",
    "    \"\"\"\n",
    "    done_flag = os.path.join(OUTPUT_DIR_FAILURE, f'batch_{batch_idx:04d}.parquet.done')\n",
    "    if os.path.exists(done_flag):\n",
    "        print(f\"[SKIP] Batch {batch_idx} already processed.\")\n",
    "        return\n",
    "    try:\n",
    "        process_batch(batch_idx)\n",
    "        open(done_flag, 'a').close()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process batch {batch_idx}: {e}\")\n",
    "\n",
    "\n",
    "def process_batch(batch_idx):\n",
    "    \"\"\"\n",
    "    Loads batch, labels circulatory failure events, and saves results.\n",
    "    Failure definition is based on vasopressor usage or MBP < 65 mmHg and lactate >= 2 mmol/L.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import gc\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n[INFO] Processing batch {batch_idx}...\")\n",
    "\n",
    "    path = os.path.join(OUTPUT_DIR_MERGED, f'batch_{batch_idx:04d}.parquet')\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[ERROR] Batch {batch_idx} not found.\")\n",
    "        return\n",
    "\n",
    "    # Load merged data with vitals and labs\n",
    "    df = pd.read_parquet(path)\n",
    "    df['charttime'] = pd.to_datetime(df['charttime'])\n",
    "    df['stay_id'] = df['stay_id'].astype(int)\n",
    "\n",
    "    # Get vasopressor administration times for relevant stays\n",
    "    stays = df['stay_id'].dropna().unique().astype(int)\n",
    "    stays_str = \",\".join(map(str, stays))\n",
    "    query = f\"\"\"\n",
    "    SELECT stay_id, starttime, endtime, {', '.join(vasopressor_cols)}\n",
    "    FROM mimiciv_derived.vasoactive_agent\n",
    "    WHERE stay_id IN ({stays_str});\n",
    "    \"\"\"\n",
    "    df_vaso = pd.read_sql(query, conn)\n",
    "    df_vaso['starttime'] = pd.to_datetime(df_vaso['starttime'])\n",
    "    df_vaso['endtime'] = pd.to_datetime(df_vaso['endtime'])\n",
    "    df_vaso['stay_id'] = df_vaso['stay_id'].astype(int)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for sid in stays:\n",
    "        df_sid = df[df['stay_id'] == sid].copy()\n",
    "        df_vsid = df_vaso[df_vaso['stay_id'] == sid]\n",
    "\n",
    "        # Initialize flags\n",
    "        df_sid['vasopressor_active'] = 0\n",
    "        df_sid['falencia'] = 0  # circulatory failure\n",
    "\n",
    "        # Annotate periods with active vasopressors\n",
    "        if not df_vsid.empty:\n",
    "            for _, row in df_vsid.iterrows():\n",
    "                if row[vasopressor_cols].notna().any():\n",
    "                    mask = (df_sid['charttime'] >= row['starttime']) & (df_sid['charttime'] <= row['endtime'])\n",
    "                    df_sid.loc[mask, 'vasopressor_active'] = 1\n",
    "\n",
    "        # Apply circulatory failure rule\n",
    "        mbp_low = df_sid['mbp'] < 65\n",
    "        lactate_high = df_sid.get('lab_50813', pd.Series([0] * len(df_sid))) >= 2\n",
    "        df_sid.loc[(mbp_low) | (df_sid['vasopressor_active'] & lactate_high), 'falencia'] = 1\n",
    "\n",
    "        results.append(df_sid)\n",
    "\n",
    "    if not results:\n",
    "        print(f\"[WARN] No data found for batch {batch_idx}.\")\n",
    "        return\n",
    "\n",
    "    df_final = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # Save final labeled batch as Parquet\n",
    "    out_parquet = os.path.join(OUTPUT_DIR_FAILURE, f'batch_{batch_idx:04d}.parquet')\n",
    "    df_final.to_parquet(out_parquet)\n",
    "\n",
    "    # Save sample for inspection (merged general + positive class)\n",
    "    sample_all = df_final.head(5000)\n",
    "    sample_failure = df_final[df_final['falencia'] == 1].head(5000)\n",
    "    sample_combined = pd.concat([sample_all, sample_failure]).drop_duplicates()\n",
    "    sample_path = os.path.join(OUTPUT_DIR_FAILURE, f'batch_{batch_idx:04d}_sample.csv')\n",
    "    sample_combined.to_csv(sample_path, index=False)\n",
    "\n",
    "    print(f\"[OK] Batch {batch_idx} saved with {len(df_final)} rows.\")\n",
    "    print(f\"[SAMPLE] Saved to {sample_path}\")\n",
    "    print(f\"[TIME] Elapsed: {time.time() - start_time:.2f} seconds\")\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# Run sequentially over all merged batches\n",
    "merged_files = sorted([\n",
    "    f for f in os.listdir(OUTPUT_DIR_MERGED)\n",
    "    if f.endswith('.parquet') and f.startswith('batch_')\n",
    "])\n",
    "batches = [int(f.split('_')[1].split('.')[0]) for f in merged_files]\n",
    "print(f\"[INFO] {len(batches)} batches found in {OUTPUT_DIR_MERGED}\")\n",
    "\n",
    "for batch in batches:\n",
    "    process_batch_if_needed(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe90068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output directories\n",
    "INPUT_DIR = f\"{OUTPUT_ROOT}/preprocess/batches/falencia\"\n",
    "OUTPUT_DIR = f\"{OUTPUT_ROOT}/preprocess/batches/falencia_split\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def split_batch_by_stay(batch_idx, stays_per_subbatch=1000):\n",
    "    \"\"\"\n",
    "    Split a large batch (parquet file) into smaller chunks based on stay_id.\n",
    "\n",
    "    Parameters:\n",
    "        batch_idx (int): Index of the batch file to process.\n",
    "        stays_per_subbatch (int): Max number of unique stay_ids per subbatch.\n",
    "    \"\"\"\n",
    "    input_path = os.path.join(INPUT_DIR, f'batch_{batch_idx:04d}.parquet')\n",
    "    print(f\"[LOAD] Reading: {input_path}\")\n",
    "\n",
    "    df = pd.read_parquet(input_path)\n",
    "\n",
    "    # Get all unique stay_ids and split them evenly\n",
    "    stay_ids = df['stay_id'].unique()\n",
    "    parts = np.array_split(stay_ids, len(stay_ids) // stays_per_subbatch + 1)\n",
    "\n",
    "    # Save each sub-batch to a separate parquet file\n",
    "    for i, ids in enumerate(parts):\n",
    "        df_sub = df[df['stay_id'].isin(ids)].copy()\n",
    "        output_path = os.path.join(OUTPUT_DIR, f'batch_{batch_idx:04d}_sub{i:02d}.parquet')\n",
    "        df_sub.to_parquet(output_path)\n",
    "        print(f\"[OK] Sub-batch {i:02d} saved with {len(df_sub)} rows and {len(ids)} stay_ids.\")\n",
    "\n",
    "# Automatically find all processed batches (excluding samples)\n",
    "batch_files = sorted([\n",
    "    f for f in os.listdir(INPUT_DIR)\n",
    "    if f.endswith('.parquet') and f.startswith('batch_') and '_amostra' not in f\n",
    "])\n",
    "batches = [int(f.split('_')[1].split('.')[0]) for f in batch_files]\n",
    "\n",
    "# Process each batch\n",
    "for batch_idx in batches:\n",
    "    split_batch_by_stay(batch_idx, stays_per_subbatch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b192ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output directories\n",
    "INPUT_DIR = f\"{OUTPUT_ROOT}/preprocess/batches/falencia_split\"\n",
    "OUTPUT_DIR = f\"{OUTPUT_ROOT}/preprocess/batches/features_raw\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def extract_features(df, batch_idx):\n",
    "    \"\"\"\n",
    "    Extracts time-series statistical features for each stay_id in the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input time-series data.\n",
    "        batch_idx (str): Identifier for the batch being processed (used for saving).\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['stay_id', 'charttime']).copy()\n",
    "\n",
    "    # Select only relevant variables (excluding identifiers, flags, and imputation columns)\n",
    "    variables = [\n",
    "        col for col in df.columns\n",
    "        if col not in ['stay_id', 'charttime', 'vasopressor_ativo', 'falencia']\n",
    "        and not col.endswith('_imputed')\n",
    "    ]\n",
    "\n",
    "    print(f\"Processing {len(variables)} variables for feature extraction.\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for sid, group in tqdm(df.groupby('stay_id'), desc=f\"Batch {batch_idx}\"):\n",
    "        group = group.sort_values('charttime')\n",
    "        feature_list = []\n",
    "\n",
    "        for var in variables:\n",
    "            try:\n",
    "                n_meas = pd.notna(group[var]).astype(int).cumsum()\n",
    "                min_val = group[var].cummin()\n",
    "                max_val = group[var].cummax()\n",
    "                mean_val = group[var].cumsum().fillna(0) / n_meas.replace(0, np.nan)\n",
    "                instab = group[var].rolling(window=12, min_periods=1).std()\n",
    "                intens = group[var].diff().abs()\n",
    "                cumul = group[var].fillna(0).cumsum()\n",
    "\n",
    "                df_var = pd.DataFrame({\n",
    "                    f'n_meas_{var}': n_meas,\n",
    "                    f'min_{var}': min_val,\n",
    "                    f'max_{var}': max_val,\n",
    "                    f'mean_{var}': mean_val,\n",
    "                    f'{var}_instab': instab,\n",
    "                    f'{var}_intens': intens,\n",
    "                    f'{var}_cumul': cumul,\n",
    "                })\n",
    "\n",
    "                feature_list.append(df_var)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing variable {var} for stay_id {sid}: {e}\")\n",
    "\n",
    "        if feature_list:\n",
    "            df_features = pd.concat(feature_list, axis=1)\n",
    "            group = pd.concat([group.reset_index(drop=True), df_features.reset_index(drop=True)], axis=1)\n",
    "\n",
    "        results.append(group)\n",
    "        del group, df_features, feature_list\n",
    "        gc.collect()\n",
    "\n",
    "    df_final = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # Save full feature-enhanced batch\n",
    "    output_path = os.path.join(OUTPUT_DIR, f'{batch_idx}.parquet')\n",
    "    df_final.to_parquet(output_path)\n",
    "\n",
    "    # Save a 10k-row checkpoint as CSV for quick inspection\n",
    "    checkpoint_path = os.path.join(OUTPUT_DIR, f'{batch_idx}_checkpoint.csv')\n",
    "    df_final.head(10000).to_csv(checkpoint_path, index=False)\n",
    "\n",
    "    print(f\"Saved full batch: {output_path}\")\n",
    "    print(f\"Saved 10k-row checkpoint: {checkpoint_path}\")\n",
    "    del df_final\n",
    "    gc.collect()\n",
    "\n",
    "def process_feature_batch(filename):\n",
    "    \"\"\"\n",
    "    Reads a single batch file and applies feature extraction.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): File name of the .parquet batch to process.\n",
    "    \"\"\"\n",
    "    path = os.path.join(INPUT_DIR, filename)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"File not found: {filename}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nProcessing feature extraction for: {filename}\")\n",
    "    df = pd.read_parquet(path)\n",
    "    print(f\"File loaded with {len(df)} rows.\")\n",
    "\n",
    "    batch_id = os.path.splitext(os.path.basename(filename))[0]\n",
    "    extract_features(df, batch_id)\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "# Entry point: process all batch files in input directory\n",
    "if __name__ == '__main__':\n",
    "    batch_files = sorted([\n",
    "        f for f in os.listdir(INPUT_DIR)\n",
    "        if f.endswith('.parquet') and f.startswith('batch_')\n",
    "    ])\n",
    "    print(f\"{len(batch_files)} batch files found for feature extraction.\")\n",
    "\n",
    "    for batch_file in batch_files:\n",
    "        process_feature_batch(batch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc695986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for input (raw feature batches) and output (filtered)\n",
    "INPUT_DIR = f\"{OUTPUT_ROOT}/preprocess/batches/features_raw\"\n",
    "OUTPUT_DIR = f\"{OUTPUT_ROOT}/preprocess/batches/features_filtered\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Step 1: Calculate global missing percentage per column\n",
    "print(\"Calculating global missing percentage...\")\n",
    "\n",
    "null_counts = {}\n",
    "total_rows = 0\n",
    "\n",
    "# List all input parquet files\n",
    "files = sorted([\n",
    "    f for f in os.listdir(INPUT_DIR)\n",
    "    if f.endswith('.parquet') and f.startswith('batch_')\n",
    "])\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_parquet(os.path.join(INPUT_DIR, file))\n",
    "    total_rows += len(df)\n",
    "    for col in df.columns:\n",
    "        null_counts[col] = null_counts.get(col, 0) + df[col].isna().sum()\n",
    "\n",
    "# Identify columns to drop (over 50% missing)\n",
    "missing_percent = {col: null_counts[col] / total_rows for col in null_counts}\n",
    "cols_to_drop = [\n",
    "    col for col, pct in missing_percent.items()\n",
    "    if pct > 0.5 and col not in ['stay_id', 'charttime']\n",
    "]\n",
    "\n",
    "print(f\"{len(cols_to_drop)} columns will be removed (over 50% missing):\")\n",
    "for col in cols_to_drop:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Step 2: Filter and save each file without the critical columns\n",
    "print(\"\\nSaving filtered files...\")\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_parquet(os.path.join(INPUT_DIR, file))\n",
    "    df_filtered = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n",
    "    df_filtered.to_parquet(os.path.join(OUTPUT_DIR, file))\n",
    "    print(f\"{file} saved (filtered).\")\n",
    "\n",
    "print(\"\\nFiltering completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_DIR = Path(OUTPUT_ROOT) / \"batches\" / \"features_filtered\"\n",
    "\n",
    "# Identify all feature files excluding label files\n",
    "parts = sorted([\n",
    "    f for f in FEATURE_DIR.glob(\"batch_*.parquet\")\n",
    "    if \"_labels\" not in f.stem\n",
    "])\n",
    "\n",
    "def generate_labels_for_part(part_path):\n",
    "    \"\"\"\n",
    "    Extracts and saves label-related columns for a given feature file.\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(part_path)\n",
    "\n",
    "    # Ensure charttime is datetime and calculate relative time in seconds\n",
    "    df['charttime'] = pd.to_datetime(df['charttime'])\n",
    "    df['rel_charttime'] = (df['charttime'] - df.groupby('stay_id')['charttime'].transform('min')).dt.total_seconds()\n",
    "\n",
    "    # Extract columns for label set\n",
    "    df_label = df[['stay_id', 'rel_charttime', 'falencia', 'vasopressor_ativo']]\n",
    "\n",
    "    # Save label file alongside original\n",
    "    label_path = part_path.parent / f\"{part_path.stem}_labels.parquet\"\n",
    "    df_label.to_parquet(label_path, index=False)\n",
    "    return label_path\n",
    "\n",
    "# Generate all label files\n",
    "label_paths = [generate_labels_for_part(p) for p in parts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a49f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_DIR = Path(OUTPUT_ROOT) / \"batches\" / \"features_filtered\"\n",
    "\n",
    "# List all feature and label files\n",
    "parts = sorted([\n",
    "    f for f in FEATURE_DIR.glob(\"batch_*.parquet\")\n",
    "    if \"_labels\" not in f.stem\n",
    "])\n",
    "labels = sorted([\n",
    "    f for f in FEATURE_DIR.glob(\"batch_*_labels.parquet\")\n",
    "])\n",
    "\n",
    "# Load one file as reference to inspect columns\n",
    "example_df = pd.read_parquet(parts[0])\n",
    "\n",
    "# Define columns to exclude from modeling\n",
    "exclude_cols = ['stay_id', 'charttime', 'falencia', 'vasopressor_ativo']\n",
    "exclude_cols += [col for col in example_df.columns if col.endswith('_imputed')]\n",
    "\n",
    "# Define final set of feature columns\n",
    "output_cols = [col for col in example_df.columns if col not in exclude_cols]\n",
    "print(f\"Total selected feature columns: {len(output_cols)}\")\n",
    "print(output_cols)\n",
    "\n",
    "# Save selected feature column names\n",
    "pd.Series(output_cols).to_csv(Path(OUTPUT_ROOT) / \"preprocess\" / \"features_names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49dfdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ml(\n",
    "    save_path,\n",
    "    parts,\n",
    "    labels,\n",
    "    endpoint_names,\n",
    "    output_cols,\n",
    "    fill_string='ffill',\n",
    "    split_path=None,\n",
    "    random_seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares time series data in HDF5 format for ML training using a standard time grid,\n",
    "    pre-normalization, and label alignment. Supports optional split file or random split.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Split helpers\n",
    "    # ---------------------------------------------------------\n",
    "    def get_splits(df, split_path, random_seed):\n",
    "        if split_path:\n",
    "            split_df = pd.read_csv(split_path, sep='\\t')\n",
    "            return {\n",
    "                split: split_df.loc[split_df['split'] == split, 'stay_id'].values\n",
    "                for split in split_df['split'].unique()\n",
    "            }\n",
    "        else:\n",
    "            all_ids = np.unique(df['stay_id'])\n",
    "            train_val, test = train_test_split(all_ids, test_size=0.15, random_state=random_seed)\n",
    "            train, val = train_test_split(train_val, test_size=0.1765, random_state=random_seed)\n",
    "            return {'train': train, 'val': val, 'test': test}\n",
    "\n",
    "    def get_windows_split(df_split, offset=0):\n",
    "        pid_array = df_split['stay_id']\n",
    "        starts = sorted(np.unique(pid_array, return_index=True)[1])\n",
    "        stops = np.concatenate([starts[1:], [df_split.shape[0]]])\n",
    "        ids = pid_array.values[starts]\n",
    "        return np.stack([np.array(starts) + offset, np.array(stops) + offset, ids], axis=1)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # HDF5 write helpers\n",
    "    # ---------------------------------------------------------\n",
    "    def save_to_h5_incremental(f, group_name, split, data, chunk_size=100_000):\n",
    "        if not f.__contains__('/data'):\n",
    "            n_data = f.create_group(\"/\", 'data', 'Dataset')\n",
    "        else:\n",
    "            n_data = f.get_node('/data')\n",
    "\n",
    "        if not n_data.__contains__(split):\n",
    "            atom = tables.Atom.from_dtype(data.dtype)\n",
    "            ea = f.create_earray(n_data, split, atom=atom, shape=(0, data.shape[1]), expectedrows=10**7)\n",
    "        else:\n",
    "            ea = n_data.get_node(split)\n",
    "\n",
    "        for i in range(0, len(data), chunk_size):\n",
    "            ea.append(data[i:i+chunk_size])\n",
    "\n",
    "    def save_labels_incremental(f, task_names, split, labels_array, chunk_size=100_000):\n",
    "        if not f.__contains__('/labels'):\n",
    "            labels_group = f.create_group(\"/\", 'labels', 'Labels')\n",
    "            f.create_array(labels_group, 'tasks', obj=[str(k).encode('utf-8') for k in task_names])\n",
    "        else:\n",
    "            labels_group = f.get_node('/labels')\n",
    "\n",
    "        if not labels_group.__contains__(split):\n",
    "            atom = tables.Atom.from_dtype(labels_array.dtype)\n",
    "            ea = f.create_earray(labels_group, split, atom=atom, shape=(0, labels_array.shape[1]), expectedrows=10**7)\n",
    "        else:\n",
    "            ea = labels_group.get_node(split)\n",
    "\n",
    "        for i in range(0, len(labels_array), chunk_size):\n",
    "            ea.append(labels_array[i:i+chunk_size])\n",
    "\n",
    "    def save_windows_incremental(f, split, windows_array, chunk_size=100_000):\n",
    "        if not f.__contains__('/patient_windows'):\n",
    "            pw_group = f.create_group(\"/\", 'patient_windows', 'Windows')\n",
    "        else:\n",
    "            pw_group = f.get_node('/patient_windows')\n",
    "\n",
    "        if not pw_group.__contains__(split):\n",
    "            atom = tables.Atom.from_dtype(windows_array.dtype)\n",
    "            ea = f.create_earray(pw_group, split, atom=atom, shape=(0, windows_array.shape[1]), expectedrows=10**7)\n",
    "        else:\n",
    "            ea = pw_group.get_node(split)\n",
    "\n",
    "        for i in range(0, len(windows_array), chunk_size):\n",
    "            ea.append(windows_array[i:i+chunk_size])\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Split setup\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"🔍 Generating data splits...\")\n",
    "    df_all_ids = pd.read_parquet(parts[0], columns=[\"stay_id\", \"charttime\"])\n",
    "    split_ids = get_splits(df_all_ids, split_path, random_seed)\n",
    "    print(f\"✅ Splits created: { {k: len(v) for k, v in split_ids.items()} }\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Compute mean and std for normalization\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"📊 Computing global mean/std for normalization...\")\n",
    "    n_total = 0\n",
    "    mean_total = None\n",
    "    M2 = None\n",
    "\n",
    "    for p in parts:\n",
    "        df = pd.read_parquet(p, columns=output_cols)\n",
    "        df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        batch_n = len(df)\n",
    "        batch_mean = df.mean()\n",
    "        batch_var = df.var(ddof=0)\n",
    "\n",
    "        if mean_total is None:\n",
    "            mean_total = batch_mean\n",
    "            M2 = batch_var * batch_n\n",
    "        else:\n",
    "            delta = batch_mean - mean_total\n",
    "            mean_total += delta * batch_n / (n_total + batch_n)\n",
    "            M2 += batch_var * batch_n + (delta ** 2) * (n_total * batch_n) / (n_total + batch_n)\n",
    "\n",
    "        n_total += batch_n\n",
    "\n",
    "    means = mean_total\n",
    "    stds = (M2 / n_total).apply(np.sqrt)\n",
    "    print(\"✅ Normalization parameters computed.\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Process and write HDF5\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"🔄 Saving to HDF5 incrementally...\")\n",
    "    offset = {'train': 0, 'val': 0, 'test': 0}\n",
    "\n",
    "    with tables.open_file(save_path, 'w') as f:\n",
    "        for i, (p, l) in enumerate(zip(parts, labels)):\n",
    "            print(f\"📦 Processing batch {i+1}/{len(parts)}: {p.name}\")\n",
    "\n",
    "            df = pd.read_parquet(p)\n",
    "            df_label = pd.read_parquet(l).rename(columns={'rel_charttime': 'charttime'})\n",
    "\n",
    "            df['charttime'] = (pd.to_datetime(df['charttime']) - pd.Timestamp(\"1970-01-01\")) / pd.Timedelta(minutes=1)\n",
    "            df_label['charttime'] = pd.to_datetime(df_label['charttime'], errors='coerce')\n",
    "\n",
    "            # Normalize features\n",
    "            df[output_cols] = (df[output_cols] - means) / stds\n",
    "            df = df.fillna(0.0)\n",
    "            df_label = df_label.fillna(0.0)\n",
    "\n",
    "            df_label = df_label[['stay_id', 'falencia']].copy()\n",
    "            df_label['falencia'] = df_label['falencia'].astype(np.float32).clip(0, 1)\n",
    "\n",
    "            for split in ['train', 'val', 'test']:\n",
    "                df_split = df[df['stay_id'].isin(split_ids[split])]\n",
    "                df_label_split = df_label[df_label['stay_id'].isin(split_ids[split])]\n",
    "\n",
    "                if df_split.empty:\n",
    "                    continue\n",
    "\n",
    "                # Handle missing columns\n",
    "                missing_cols = [c for c in output_cols if c not in df_split.columns]\n",
    "                for c in missing_cols:\n",
    "                    df_split[c] = 0.0\n",
    "\n",
    "                win = get_windows_split(df_split, offset=offset[split])\n",
    "                features_array = df_split[output_cols].astype(np.float32).values\n",
    "                labels_array = df_label_split.drop(columns=['stay_id']).values.astype(np.float32)\n",
    "\n",
    "                save_to_h5_incremental(f, 'data', split, features_array)\n",
    "                save_labels_incremental(f, endpoint_names, split, labels_array)\n",
    "                save_windows_incremental(f, split, win.astype(np.int32))\n",
    "\n",
    "                offset[split] += df_split.shape[0]\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"✅ Finished! Saved to {save_path}\")\n",
    "\n",
    "to_ml(\n",
    "    save_path=Path(OUTPUT_ROOT) / \"h5\" / \"dataset.h5\",\n",
    "    parts=parts,\n",
    "    labels=labels,\n",
    "    endpoint_names=[\"falencia\"],\n",
    "    output_cols=output_cols,\n",
    "    fill_string=\"ffill\",\n",
    "    split_path=None,\n",
    "    random_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f36374",
   "metadata": {},
   "source": [
    "## 2) Expected outputs\n",
    "- batches em `OUT_DIR` (features_filtered, falencia, etc.)\n",
    "- HDF5 em `OUT_DIR/h5/dataset.h5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8915a11",
   "metadata": {},
   "source": [
    "## Build HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a88a90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build HDF5 (MIMIC)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables\n",
    "\n",
    "BATCH_DIR = Path(OUT_DIR) / \"batches\" / \"features_filtered\"\n",
    "H5_PATH = Path(OUT_DIR) / \"h5\" / \"dataset_mimic.h5\"\n",
    "SPLIT_TSV = Path(OUT_DIR) / \"split_all.tsv\"  # optional; will be created if missing\n",
    "\n",
    "ID_COL = \"stay_id\"\n",
    "TIME_COL = \"charttime\"\n",
    "LABEL_COL = \"falencia\"\n",
    "SEED = 42\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC = 0.1\n",
    "\n",
    "# --- helpers ---\n",
    "def ensure_groups(h5, groups):\n",
    "    for g in groups:\n",
    "        if f\"/{g}\" not in h5:\n",
    "            h5.create_group(\"/\", g, f\"{g} group\")\n",
    "\n",
    "\n",
    "def create_earray(h5, path, atom, n_cols, expectedrows=10_000_000):\n",
    "    parent, name = path.rsplit(\"/\", 1)\n",
    "    return h5.create_earray(parent, name, atom=atom, shape=(0, n_cols), expectedrows=expectedrows)\n",
    "\n",
    "\n",
    "def build_split(stay_ids, seed, train_frac, val_frac):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    stay_ids = np.array(sorted(stay_ids), dtype=int)\n",
    "    rng.shuffle(stay_ids)\n",
    "    n = len(stay_ids)\n",
    "    n_train = int(n * train_frac)\n",
    "    n_val = int(n * val_frac)\n",
    "    return {\n",
    "        \"train\": set(stay_ids[:n_train].tolist()),\n",
    "        \"val\": set(stay_ids[n_train:n_train + n_val].tolist()),\n",
    "        \"test\": set(stay_ids[n_train + n_val:].tolist()),\n",
    "    }\n",
    "\n",
    "\n",
    "def load_split_map(split_tsv, stay_ids, seed, train_frac, val_frac):\n",
    "    if split_tsv.exists():\n",
    "        df = pd.read_csv(split_tsv, sep=\"\t\")\n",
    "        if {ID_COL, \"split\"}.issubset(df.columns):\n",
    "            return dict(zip(df[ID_COL].astype(int), df[\"split\"]))\n",
    "    split_sets = build_split(stay_ids, seed, train_frac, val_frac)\n",
    "    split_tsv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with split_tsv.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{ID_COL}\\tsplit\\n\")\n",
    "        for split, ids in split_sets.items():\n",
    "            for sid in sorted(ids):\n",
    "                f.write(f\"{sid}\\t{split}\\n\")\n",
    "    return {sid: split for split, ids in split_sets.items() for sid in ids}\n",
    "\n",
    "\n",
    "# --- run ---\n",
    "paths = sorted(p for p in BATCH_DIR.glob(\"batch_*.parquet\") if not p.name.endswith(\"_labels.parquet\"))\n",
    "if not paths:\n",
    "    raise SystemExit(f\"No batch_*.parquet found in {BATCH_DIR}\")\n",
    "\n",
    "# collect stays\n",
    "stay_ids = set()\n",
    "for p in paths:\n",
    "    df_ids = pd.read_parquet(p, columns=[ID_COL])\n",
    "    stay_ids.update(df_ids[ID_COL].dropna().astype(int).unique().tolist())\n",
    "\n",
    "split_map = load_split_map(SPLIT_TSV, stay_ids, SEED, TRAIN_FRAC, VAL_FRAC)\n",
    "\n",
    "# infer feature columns\n",
    "first_df = pd.read_parquet(paths[0])\n",
    "drop_cols = {ID_COL, TIME_COL, LABEL_COL}\n",
    "feature_cols = [c for c in first_df.columns if c not in drop_cols]\n",
    "\n",
    "H5_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "if H5_PATH.exists():\n",
    "    H5_PATH.unlink()\n",
    "\n",
    "with tables.open_file(H5_PATH, mode=\"w\") as h5:\n",
    "    ensure_groups(h5, [\"data\", \"labels\", \"patient_windows\"])\n",
    "    data_arrays = {}\n",
    "    label_arrays = {}\n",
    "    window_arrays = {}\n",
    "    stay_id_arrays = {}\n",
    "\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        data_arrays[split] = create_earray(h5, f\"/data/{split}\", tables.Float32Atom(), n_cols=len(feature_cols))\n",
    "        label_arrays[split] = create_earray(h5, f\"/labels/{split}\", tables.Float32Atom(), n_cols=1)\n",
    "        window_arrays[split] = create_earray(h5, f\"/patient_windows/{split}\", tables.Int32Atom(), n_cols=3)\n",
    "        stay_id_arrays[split] = h5.create_earray(\"/patient_windows\", f\"{split}_stay_ids\", atom=tables.Int32Atom(), shape=(0,))\n",
    "\n",
    "    for idx, path in enumerate(paths, start=1):\n",
    "        df = first_df if idx == 1 else pd.read_parquet(path)\n",
    "        if TIME_COL in df.columns:\n",
    "            df = df.sort_values([ID_COL, TIME_COL])\n",
    "        else:\n",
    "            df = df.sort_values([ID_COL])\n",
    "        df[ID_COL] = df[ID_COL].astype(int)\n",
    "\n",
    "        for stay_id, df_sid in df.groupby(ID_COL, sort=False):\n",
    "            split = split_map.get(int(stay_id), \"train\")\n",
    "            d_arr = data_arrays[split]\n",
    "            l_arr = label_arrays[split]\n",
    "            w_arr = window_arrays[split]\n",
    "            sid_arr = stay_id_arrays[split]\n",
    "\n",
    "            start = d_arr.nrows\n",
    "            feat = df_sid[feature_cols].astype(np.float32).to_numpy()\n",
    "            lbl = df_sid[[LABEL_COL]].astype(np.float32).to_numpy() if LABEL_COL in df_sid.columns else np.zeros((len(df_sid), 1), dtype=np.float32)\n",
    "            d_arr.append(feat)\n",
    "            l_arr.append(lbl)\n",
    "            stop = d_arr.nrows\n",
    "\n",
    "            w_arr.append(np.array([[start, stop, int(stay_id)]], dtype=np.int32))\n",
    "            sid_arr.append(np.array([int(stay_id)], dtype=np.int32))\n",
    "\n",
    "        if idx % 10 == 0 or idx == 1 or idx == len(paths):\n",
    "            print(f\"[{idx}/{len(paths)}] {path.name}\")\n",
    "\n",
    "    h5.set_node_attr(\"/\", \"feature_names\", feature_cols)\n",
    "    h5.set_node_attr(\"/\", \"label_column\", LABEL_COL)\n",
    "\n",
    "print(\"H5 saved to\", H5_PATH, \"with\", len(feature_cols), \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d52eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build per-stay falencia summary (falencia_normal + 45/60min + mortality)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "PREPROCESS_DIR = OUT_DIR / \"preprocess\"\n",
    "INPUT_DIR = PREPROCESS_DIR / \"batches\" / \"features_filtered\"\n",
    "OUT_CSV = PREPROCESS_DIR / \"falencia_stay_summary.csv\"\n",
    "MORTALITY_PATH = PREPROCESS_DIR / \"mortality_by_stay.csv\"\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    str(Path(\"src/clustering/minirocket/pipelines/build_falencia_stay_summary.py\")),\n",
    "    \"--input_dir\", str(INPUT_DIR),\n",
    "    \"--mortality_path\", str(MORTALITY_PATH),\n",
    "    \"--out_csv\", str(OUT_CSV),\n",
    "    \"--time_col\", \"charttime\",\n",
    "    \"--id_col\", \"stay_id\",\n",
    "    \"--mbp_col\", \"mbp\",\n",
    "    \"--vaso_col\", \"vasopressor_ativo\",\n",
    "    \"--lactate_col\", \"lab_50813\",\n",
    "    \"--falencia_col\", \"falencia\",\n",
    "    \"--step_min_45\", \"5\", \"--window_min_45\", \"45\",\n",
    "    \"--step_min_60\", \"60\", \"--window_min_60\", \"60\",\n",
    "]\n",
    "print(\" \".join(cmd))\n",
    "subprocess.run(cmd, check=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
