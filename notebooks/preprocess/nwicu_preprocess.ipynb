{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "153f8976",
   "metadata": {},
   "source": [
    "# NWICU Preprocess (standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471906e",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56736e1a",
   "metadata": {},
   "source": [
    "### 0.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd922eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from __future__ import annotations\n",
    "from typing import List\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa156b6",
   "metadata": {},
   "source": [
    "### 0.2 Paths & environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d17cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Ajuste estes caminhos\n",
    "BASE_DIR = Path(r\"<DATA_DIR>/nwicu\")  # raiz com nw_icu/csv e nw_hosp/csv\n",
    "OUT_DIR = Path(r\"<OUTPUT_DIR>/nwicu\")  # diret?rio de sa?da\n",
    "\n",
    "PREPROCESS_DIR = OUT_DIR / \"preprocess\"\n",
    "IMPUTED_DIR = PREPROCESS_DIR / \"imputed_full\"\n",
    "LABELS_PATH = PREPROCESS_DIR / \"labels.parquet\"\n",
    "FEATURES_DIR = PREPROCESS_DIR / \"batches\" / \"features_raw\"\n",
    "H5_PATH = PREPROCESS_DIR / \"h5\" / \"dataset_features.h5\"\n",
    "\n",
    "PREPROCESS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d558690",
   "metadata": {},
   "source": [
    "### 0.3 Defaults / mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0050e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VITALS_MAP = {320045: 'heart_rate', 320179: 'sbp', 320180: 'dbp', 320050: 'sbp_art', 320051: 'dbp_art', 320210: 'resp_rate', 320277: 'spo2', 323761: 'temperature_f', 326707: 'height_in', 326531: 'weight_oz', 300001: 'bmi'}\n",
    "LABS_MAP = {100001: 'glucose', 100013: 'lab_50803', 100010: 'lab_50983', 100011: 'lab_50971', 100015: 'lab_50970', 100044: 'lab_50808', 100031: 'lab_50813', 100020: 'lab_50885', 100049: 'lab_50883', 100042: 'lab_50861', 100032: 'lab_50878', 100053: 'lab_50889', 100021: 'lab_50862', 100006: 'lab_51638', 100007: 'lab_50855', 100079: 'lab_50852', 100349: 'lab_51223', 100014: 'platelet_count', 100030: 'lab_51274', 100046: 'lab_51275', 100048: 'lab_51214', 100145: 'lab_52144', 100359: 'lab_51647', 100085: 'lab_50910', 100285: 'lab_50908', 100059: 'lab_51002', 100057: 'lab_51003', 100071: 'lab_50963', 100002: 'lab_50912', 100004: 'lab_51006', 100009: 'lab_50960', 100209: 'lab_51099', 100075: 'lab_50915', 100221: 'lab_50966', 100216: 'lab_50967', 100223: 'lab_50975', 100052: 'lab_50924', 100123: 'lab_50935', 100150: 'lab_50805', 100134: 'lab_50856'}\n",
    "VASO_REGEX = 'norepinephrine|epinephrine|phenylephrine|vasopressin|dopamine|dobutamine|milrinone'\n",
    "MAP_THRESHOLD = 65.0\n",
    "LACTATE_THRESHOLD = 2.0\n",
    "GRID_MINUTES = 5\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    VITALS_MAP=VITALS_MAP,\n",
    "    LABS_MAP=LABS_MAP,\n",
    "    VASO_REGEX=VASO_REGEX,\n",
    "    MAP_THRESHOLD=MAP_THRESHOLD,\n",
    "    LACTATE_THRESHOLD=LACTATE_THRESHOLD,\n",
    "    GRID_MINUTES=GRID_MINUTES,\n",
    "    BASE_DIR=BASE_DIR,\n",
    "    OUTPUT_DIR=OUT_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pré-processamento: mapeia itemids, pivot vitais/labs e marca vasopressor.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def load_vitals(chartevents_path: Path) -> pd.DataFrame:\n",
    "    use_ids = set(config.VITALS_MAP)\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(\n",
    "        chartevents_path,\n",
    "        chunksize=500_000,\n",
    "        parse_dates=[\"charttime\"],\n",
    "        usecols=[\"stay_id\", \"charttime\", \"itemid\", \"valuenum\"],\n",
    "    ):\n",
    "        chunk = chunk[chunk[\"itemid\"].isin(use_ids)]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "        # remove sentinelas/valores absurdos\n",
    "        chunk[\"valuenum\"] = chunk[\"valuenum\"].where(chunk[\"valuenum\"].abs() < 1e6, np.nan)\n",
    "        chunk[\"feature\"] = chunk[\"itemid\"].map(config.VITALS_MAP)\n",
    "        chunk[\"charttime\"] = pd.to_datetime(chunk[\"charttime\"]).dt.tz_localize(None)\n",
    "        chunks.append(chunk[[\"stay_id\", \"charttime\", \"feature\", \"valuenum\"]])\n",
    "    if not chunks:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    wide = (\n",
    "        df.pivot_table(index=[\"stay_id\", \"charttime\"], columns=\"feature\", values=\"valuenum\")\n",
    "        .reset_index()\n",
    "        .sort_values([\"stay_id\", \"charttime\"])\n",
    "    )\n",
    "    if \"mbp\" not in wide.columns and {\"sbp\", \"dbp\"}.issubset(wide.columns):\n",
    "        wide[\"mbp\"] = (2 * wide[\"dbp\"] + wide[\"sbp\"]) / 3.0\n",
    "    if \"temperature_f\" in wide.columns and \"temperature\" not in wide.columns:\n",
    "        wide[\"temperature\"] = (wide[\"temperature_f\"] - 32.0) * 5.0 / 9.0\n",
    "    # Conversões antropométricas\n",
    "    if \"weight_oz\" in wide.columns and \"weight_kg\" not in wide.columns:\n",
    "        wide[\"weight_kg\"] = wide[\"weight_oz\"] * 0.0283495\n",
    "    if \"height_in\" in wide.columns and \"height_cm\" not in wide.columns:\n",
    "        wide[\"height_cm\"] = wide[\"height_in\"] * 2.54\n",
    "    return wide\n",
    "\n",
    "\n",
    "def load_labs(labevents_path: Path, icustays_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    use_ids = set(config.LABS_MAP)\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(\n",
    "        labevents_path,\n",
    "        chunksize=500_000,\n",
    "        parse_dates=[\"charttime\"],\n",
    "        usecols=[\"subject_id\", \"hadm_id\", \"charttime\", \"itemid\", \"valuenum\"],\n",
    "    ):\n",
    "        chunk = chunk[chunk[\"itemid\"].isin(use_ids)]\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "        # remove sentinelas/valores absurdos\n",
    "        chunk[\"valuenum\"] = chunk[\"valuenum\"].where(chunk[\"valuenum\"].abs() < 1e6, np.nan)\n",
    "        chunk = chunk.merge(icustays_df[[\"subject_id\", \"hadm_id\", \"stay_id\"]], on=[\"subject_id\", \"hadm_id\"], how=\"left\")\n",
    "        chunk = chunk.dropna(subset=[\"stay_id\"])\n",
    "        chunk[\"stay_id\"] = chunk[\"stay_id\"].astype(int)\n",
    "        chunk[\"feature\"] = chunk[\"itemid\"].map(config.LABS_MAP)\n",
    "        chunk[\"charttime\"] = pd.to_datetime(chunk[\"charttime\"]).dt.tz_localize(None)\n",
    "        chunks.append(chunk[[\"stay_id\", \"charttime\", \"feature\", \"valuenum\"]])\n",
    "    if not chunks:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    wide = (\n",
    "        df.pivot_table(index=[\"stay_id\", \"charttime\"], columns=\"feature\", values=\"valuenum\")\n",
    "        .reset_index()\n",
    "        .sort_values([\"stay_id\", \"charttime\"])\n",
    "    )\n",
    "    return wide\n",
    "\n",
    "\n",
    "def load_vaso_flags(prescriptions_path: Path, icustays_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    pattern = re.compile(config.VASO_REGEX, flags=re.IGNORECASE)\n",
    "    df = pd.read_csv(\n",
    "        prescriptions_path,\n",
    "        usecols=[\"subject_id\", \"hadm_id\", \"starttime\", \"stoptime\", \"drug\"],\n",
    "        parse_dates=[\"starttime\", \"stoptime\"],\n",
    "    )\n",
    "    df = df[df[\"drug\"].fillna(\"\").str.contains(pattern)]\n",
    "    df = df.merge(icustays_df[[\"subject_id\", \"hadm_id\", \"stay_id\"]], on=[\"subject_id\", \"hadm_id\"], how=\"left\")\n",
    "    df = df.dropna(subset=[\"stay_id\", \"starttime\", \"stoptime\"])\n",
    "    df[\"stay_id\"] = df[\"stay_id\"].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_static(admissions_path: Path, icustays_path: Path) -> pd.DataFrame:\n",
    "    adm = pd.read_csv(\n",
    "        admissions_path,\n",
    "        usecols=[\"subject_id\", \"hadm_id\", \"deathtime\", \"hospital_expire_flag\", \"admittime\", \"dischtime\"],\n",
    "        parse_dates=[\"deathtime\", \"admittime\", \"dischtime\"],\n",
    "    )\n",
    "    icu = pd.read_csv(\n",
    "        icustays_path,\n",
    "        usecols=[\"subject_id\", \"hadm_id\", \"stay_id\", \"intime\", \"outtime\", \"los\"],\n",
    "        parse_dates=[\"intime\", \"outtime\"],\n",
    "    )\n",
    "    patients = pd.read_csv(\n",
    "        Path(admissions_path).parent / \"patients.csv\",\n",
    "        usecols=[\"subject_id\", \"gender\", \"anchor_age\", \"dod\"],\n",
    "        parse_dates=[\"dod\"],\n",
    "    )\n",
    "    static = icu.merge(adm, on=[\"subject_id\", \"hadm_id\"], how=\"left\")\n",
    "    static = static.merge(patients, on=\"subject_id\", how=\"left\")\n",
    "    static[\"mortality\"] = ((static[\"hospital_expire_flag\"] == 1) | static[\"deathtime\"].notna() | static[\"dod\"].notna()).astype(int)\n",
    "    return static[[\"stay_id\", \"subject_id\", \"hadm_id\", \"intime\", \"outtime\", \"los\", \"mortality\", \"gender\", \"anchor_age\"]]\n",
    "\n",
    "\n",
    "def prep_merge_main():\n",
    "    parser = argparse.ArgumentParser(description=\"Prepara merge de vitais/labs/vaso para NWICU.\")\n",
    "    parser.add_argument(\"--base-dir\", type=str, default=config.BASE_DIR, help=\"Pasta raiz com nw_icu/csv e nw_hosp/csv.\")\n",
    "    parser.add_argument(\"--out-dir\", type=str, default=config.OUTPUT_DIR, help=\"Diretório de saída.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    base = Path(args.base_dir)\n",
    "    out_dir = Path(args.out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    chartevents = base / \"nw_icu\" / \"csv\" / \"chartevents.csv\"\n",
    "    labevents = base / \"nw_hosp\" / \"csv\" / \"labevents.csv\"\n",
    "    prescriptions = base / \"nw_hosp\" / \"csv\" / \"prescriptions.csv\"\n",
    "    admissions = base / \"nw_hosp\" / \"csv\" / \"admissions.csv\"\n",
    "    icustays = base / \"nw_icu\" / \"csv\" / \"icustays.csv\"\n",
    "\n",
    "    icu_df = pd.read_csv(icustays, usecols=[\"subject_id\", \"hadm_id\", \"stay_id\"])\n",
    "\n",
    "    print(\"Lendo vitais...\")\n",
    "    vitals = load_vitals(chartevents)\n",
    "    print(f\"Vitais: {len(vitals)} linhas\")\n",
    "\n",
    "    print(\"Lendo labs...\")\n",
    "    labs = load_labs(labevents, icu_df)\n",
    "    print(f\"Labs: {len(labs)} linhas\")\n",
    "\n",
    "    print(\"Lendo vasopressores...\")\n",
    "    vaso = load_vaso_flags(prescriptions, icu_df)\n",
    "    print(f\"Intervalos de vaso: {len(vaso)}\")\n",
    "\n",
    "    print(\"Unindo vitais + labs...\")\n",
    "    merged = pd.merge(vitals, labs, on=[\"stay_id\", \"charttime\"], how=\"outer\")\n",
    "    merged = merged.sort_values([\"stay_id\", \"charttime\"])\n",
    "\n",
    "    if \"weight_kg\" not in merged.columns and \"weight_oz\" in merged.columns:\n",
    "        merged[\"weight_kg\"] = merged[\"weight_oz\"] * 0.0283495\n",
    "    if \"height_cm\" not in merged.columns and \"height_in\" in merged.columns:\n",
    "        merged[\"height_cm\"] = merged[\"height_in\"] * 2.54\n",
    "\n",
    "    merged[\"vasopressor_ativo\"] = 0\n",
    "    if not vaso.empty:\n",
    "        for stay_id, group in vaso.groupby(\"stay_id\"):\n",
    "            mask = merged[\"stay_id\"] == stay_id\n",
    "            if not mask.any():\n",
    "                continue\n",
    "            for _, row in group.iterrows():\n",
    "                mtime = (merged[\"charttime\"] >= row[\"starttime\"]) & (merged[\"charttime\"] <= row[\"stoptime\"])\n",
    "                merged.loc[mask & mtime, \"vasopressor_ativo\"] = 1\n",
    "\n",
    "    print(\"Salvando merged...\")\n",
    "    merged.to_parquet(out_dir / \"merged.parquet\", index=False)\n",
    "\n",
    "    print(\"Gerando estáticos...\")\n",
    "    static = load_static(admissions, icustays)\n",
    "    static.to_parquet(out_dir / \"static.parquet\", index=False)\n",
    "\n",
    "    print(\"Feito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960f6b1",
   "metadata": {},
   "source": [
    "## 1) Preprocess pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d310d",
   "metadata": {},
   "source": [
    "# NWICU preprocess (self-contained)\n",
    "\n",
    "Este notebook cont?m TODO o preprocess do NWICU/CircEWS em um ?nico fluxo, sem depender de scripts externos. Ajuste apenas os caminhos e execute em ordem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e023c",
   "metadata": {},
   "source": [
    "## 0) Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c047f8b",
   "metadata": {},
   "source": [
    "### 0.1 Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e7b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Mappings do NWICU (extra?dos de datasets/nwicu/src/config.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163aaf4c",
   "metadata": {},
   "source": [
    "### 0.2 Paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58800c1f",
   "metadata": {},
   "source": [
    "### 0.3 Defaults / mappings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0755672",
   "metadata": {},
   "source": [
    "## 2) Preprocess step: prep_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36865182",
   "metadata": {},
   "source": [
    "## 3) Preprocess step: impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac83cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imputação estilo CircularyFailure: grade 5 min, defaults fisiológicos, forward-fill e flags *_imputed.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def iter_grids(df: pd.DataFrame, grid_minutes: int):\n",
    "    freq = f\"{grid_minutes}min\"\n",
    "    for stay_id, sub in df.groupby(\"stay_id\"):\n",
    "        sub = sub.sort_values(\"charttime\").set_index(\"charttime\")\n",
    "        grid = sub.resample(freq).last()\n",
    "        yield stay_id, grid\n",
    "\n",
    "\n",
    "def impute_main():\n",
    "    parser = argparse.ArgumentParser(description=\"Imputa merged NWICU com grade fixa, defaults e flags *_imputed.\")\n",
    "    parser.add_argument(\"--merged\", type=str, required=True, help=\"Parquet merged (saída do prep_merge).\")\n",
    "    parser.add_argument(\"--static\", type=str, default=config.OUTPUT_DIR / \"preprocess\" / \"static.parquet\", help=\"Parquet static com mortality (opcional).\")\n",
    "    parser.add_argument(\"--out\", type=str, default=config.OUTPUT_DIR / \"preprocess\" / \"imputed_full\", help=\"Diretório de saída (ou base).\")\n",
    "    parser.add_argument(\"--grid-minutes\", type=int, default=config.GRID_MINUTES, help=\"Tamanho da grade (min).\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    merged = pd.read_parquet(args.merged)\n",
    "    merged[\"charttime\"] = pd.to_datetime(merged[\"charttime\"]).dt.tz_localize(None)\n",
    "\n",
    "    # Defaults globais (mediana + overrides fisiológicos)\n",
    "    numeric_cols = [c for c in merged.columns if c not in {\"stay_id\", \"charttime\"}]\n",
    "    global_defaults = merged[numeric_cols].median(skipna=True)\n",
    "    phys = {\n",
    "        \"heart_rate\": 80.0,\n",
    "        \"resp_rate\": 18.0,\n",
    "        \"sbp\": 120.0,\n",
    "        \"sbp_art\": 120.0,\n",
    "        \"dbp\": 70.0,\n",
    "        \"dbp_art\": 70.0,\n",
    "        \"mbp\": 85.0,\n",
    "        \"spo2\": 98.0,\n",
    "        \"temperature\": 36.8,\n",
    "        \"temperature_f\": 98.2,\n",
    "        \"glucose\": 110.0,\n",
    "        # labs (valores de referência aproximados)\n",
    "        \"lab_50803\": 24.0,   # bicarbonato/CO2 total\n",
    "        \"lab_50805\": 1.5,\n",
    "        \"lab_50808\": 1.1,\n",
    "        \"lab_50809\": 90.0,\n",
    "        \"lab_50813\": 1.0,\n",
    "        \"lab_50852\": 5.5,\n",
    "        \"lab_50855\": 14.0,\n",
    "        \"lab_50856\": 10.0,\n",
    "        \"lab_50861\": 30.0,\n",
    "        \"lab_50862\": 4.3,\n",
    "        \"lab_50878\": 40.0,\n",
    "        \"lab_50883\": 0.3,\n",
    "        \"lab_50884\": 0.6,\n",
    "        \"lab_50885\": 1.0,\n",
    "        \"lab_50889\": 5.0,\n",
    "        \"lab_50908\": 0.8,\n",
    "        \"lab_50910\": 150.0,\n",
    "        \"lab_50912\": 1.0,\n",
    "        \"lab_50915\": 0.5,\n",
    "        \"lab_50924\": 120.0,\n",
    "        \"lab_50928\": 100.0,\n",
    "        \"lab_50931\": 90.0,\n",
    "        \"lab_50935\": 100.0,\n",
    "        \"lab_50960\": 2.0,\n",
    "        \"lab_50963\": 100.0,\n",
    "        \"lab_50966\": 15.0,\n",
    "        \"lab_50967\": 15.0,\n",
    "        \"lab_50968\": 10.0,\n",
    "        \"lab_50969\": 80.0,\n",
    "        \"lab_50970\": 3.5,\n",
    "        \"lab_50971\": 4.5,\n",
    "        \"lab_50975\": 5.5,\n",
    "        \"lab_50983\": 140.0,\n",
    "        \"lab_50990\": 10.0,\n",
    "        \"lab_51002\": 0.01,\n",
    "        \"lab_51003\": 0.01,\n",
    "        \"lab_51006\": 15.0,\n",
    "        \"lab_51099\": 0.2,\n",
    "        \"lab_51196\": 0.5,\n",
    "        \"lab_51214\": 300.0,\n",
    "        \"lab_51222\": 14.0,\n",
    "        \"lab_51223\": 3.0,\n",
    "        \"lab_51265\": 200.0,\n",
    "        \"lab_51266\": 1.0,\n",
    "        \"lab_51274\": 12.0,\n",
    "        \"lab_51275\": 35.0,\n",
    "        \"lab_51290\": 0.0,\n",
    "        \"lab_51291\": 0.0,\n",
    "        \"lab_51292\": 0.0,\n",
    "        \"lab_51464\": 0.0,\n",
    "        \"lab_51568\": 0.2,\n",
    "        \"lab_51569\": 0.2,\n",
    "        \"lab_51570\": 0.2,\n",
    "        \"lab_51580\": 0.8,\n",
    "        \"lab_51623\": 300.0,\n",
    "        \"lab_51631\": 6.0,\n",
    "        \"lab_51638\": 45.0,\n",
    "        \"lab_51640\": 14.0,\n",
    "        \"lab_51643\": 3.0,\n",
    "        \"lab_51647\": 3.0,\n",
    "        \"lab_51966\": 0.0,\n",
    "        \"lab_52116\": 300.0,\n",
    "        \"lab_52117\": 300.0,\n",
    "        \"lab_52142\": 10.0,\n",
    "        \"lab_52144\": 1.0,\n",
    "        \"lab_52546\": 1.0,\n",
    "        \"lab_52551\": 0.5,\n",
    "        \"lab_52642\": 0.01,\n",
    "        \"lab_53085\": 4.3,\n",
    "    }\n",
    "    for k, v in phys.items():\n",
    "        if k in global_defaults:\n",
    "            global_defaults[k] = v\n",
    "    if \"weight_kg\" not in global_defaults or pd.isna(global_defaults.get(\"weight_kg\")):\n",
    "        global_defaults[\"weight_kg\"] = 70.0\n",
    "    if \"height_cm\" not in global_defaults or pd.isna(global_defaults.get(\"height_cm\")):\n",
    "        global_defaults[\"height_cm\"] = 170.0\n",
    "    if \"bmi\" not in global_defaults or pd.isna(global_defaults.get(\"bmi\")):\n",
    "        global_defaults[\"bmi\"] = 24.0\n",
    "\n",
    "    out_base = Path(args.out)\n",
    "    out_dir = out_base if not out_base.suffix else out_base.with_suffix(\"\")\n",
    "    if out_dir.exists():\n",
    "        shutil.rmtree(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    static = (\n",
    "        pd.read_parquet(args.static)[[\"stay_id\", \"mortality\"]].drop_duplicates(\"stay_id\")\n",
    "        if args.static and Path(args.static).exists()\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    batch = []\n",
    "    batch_size = 200  # stays por batch\n",
    "    total_rows = 0\n",
    "\n",
    "    for idx, (stay_id, grid) in enumerate(iter_grids(merged, args.grid_minutes), start=1):\n",
    "        grid = grid.sort_index()\n",
    "        orig_mask = grid.notna()\n",
    "        grid = grid.ffill()\n",
    "        grid = grid.fillna(global_defaults)\n",
    "        # flags\n",
    "        for col in [c for c in grid.columns if c not in {\"stay_id\"}]:\n",
    "            grid[f\"{col}_imputed\"] = (~orig_mask[col]).astype(int)\n",
    "        grid = grid.reset_index()\n",
    "        grid[\"stay_id\"] = stay_id\n",
    "        # MBP se faltar\n",
    "        if \"mbp\" not in grid.columns and {\"sbp\", \"dbp\"}.issubset(grid.columns):\n",
    "            grid[\"mbp\"] = (2 * grid[\"dbp\"] + grid[\"sbp\"]) / 3.0\n",
    "        # peso/altura/bmi coerentes\n",
    "        if \"weight_kg\" not in grid.columns and \"weight_oz\" in grid.columns:\n",
    "            grid[\"weight_kg\"] = grid[\"weight_oz\"] * 0.0283495\n",
    "        if \"height_cm\" not in grid.columns and \"height_in\" in grid.columns:\n",
    "            grid[\"height_cm\"] = grid[\"height_in\"] * 2.54\n",
    "        if \"weight_kg\" in grid.columns and \"height_cm\" in grid.columns:\n",
    "            grid[\"bmi\"] = grid.get(\"bmi\", np.nan)\n",
    "            grid[\"bmi\"] = grid[\"bmi\"].fillna(grid[\"weight_kg\"] / ((grid[\"height_cm\"] / 100.0) ** 2))\n",
    "        # mortalidade constante\n",
    "        if static is not None:\n",
    "            grid = grid.merge(static, on=\"stay_id\", how=\"left\")\n",
    "        else:\n",
    "            grid[\"mortality\"] = np.nan\n",
    "\n",
    "        # reordena: charttime, stay_id, pares col/col_imputed, depois vasopressor_ativo, mortality\n",
    "        order = [\"charttime\", \"stay_id\"]\n",
    "        base_cols = [c for c in grid.columns if not c.endswith(\"_imputed\") and c not in {\"charttime\", \"stay_id\"}]\n",
    "        for col in base_cols:\n",
    "            if col in {\"vasopressor_ativo\", \"mortality\"}:\n",
    "                continue\n",
    "            order.append(col)\n",
    "            flag = f\"{col}_imputed\"\n",
    "            if flag in grid.columns:\n",
    "                order.append(flag)\n",
    "        for tail in [\"vasopressor_ativo\", \"mortality\"]:\n",
    "            if tail in grid.columns and tail not in order:\n",
    "                order.append(tail)\n",
    "        grid = grid[order]\n",
    "\n",
    "        batch.append(grid)\n",
    "\n",
    "        if len(batch) >= batch_size:\n",
    "            df_batch = pd.concat(batch, ignore_index=True)\n",
    "            part_path = out_dir / f\"part_{idx:05d}.parquet\"\n",
    "            df_batch.to_parquet(part_path, index=False, compression=\"snappy\", engine=\"pyarrow\")\n",
    "            total_rows += len(df_batch)\n",
    "            batch.clear()\n",
    "\n",
    "    if batch:\n",
    "        df_batch = pd.concat(batch, ignore_index=True)\n",
    "        part_path = out_dir / \"part_tail.parquet\"\n",
    "        df_batch.to_parquet(part_path, index=False, compression=\"snappy\", engine=\"pyarrow\")\n",
    "        total_rows += len(df_batch)\n",
    "\n",
    "    print(f\"Imputed salvo em {out_dir} (~{total_rows} linhas, múltiplas parts).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea9f90",
   "metadata": {},
   "source": [
    "## 4) Preprocess step: label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b8be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gera labels de falência (circEWS-like) a partir do imputed.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def label_main():\n",
    "    parser = argparse.ArgumentParser(description=\"Gera labels de falência (MAP<=65 ou vaso & lactato>=2).\")\n",
    "    parser.add_argument(\"--imputed\", type=str, required=True, help=\"Parquet ou diretório com parts (saída do impute).\")\n",
    "    parser.add_argument(\"--out\", type=str, default=config.OUTPUT_DIR / \"preprocess\" / \"labels.parquet\", help=\"Arquivo de saída.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    imp_path = Path(args.imputed)\n",
    "    if imp_path.is_dir():\n",
    "        files = sorted(imp_path.glob(\"*.parquet\"))\n",
    "    else:\n",
    "        files = [imp_path]\n",
    "\n",
    "    out_path = Path(args.out)\n",
    "    if out_path.exists():\n",
    "        out_path.unlink()\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    total = 0\n",
    "    parts = []\n",
    "    for f in files:\n",
    "        df = pd.read_parquet(f)\n",
    "        df[\"charttime\"] = pd.to_datetime(df[\"charttime\"])\n",
    "        df = df.sort_values([\"stay_id\", \"charttime\"])\n",
    "\n",
    "        df[\"mbp_baixa\"] = df.get(\"mbp\", pd.Series(index=df.index, dtype=float)) <= config.MAP_THRESHOLD\n",
    "        df[\"lactato_alto\"] = df.get(\"lab_50813\", pd.Series(index=df.index, dtype=float)) >= config.LACTATE_THRESHOLD\n",
    "        df[\"vasopressor_ativo\"] = df.get(\"vasopressor_ativo\", pd.Series(index=df.index, dtype=float)).fillna(0) > 0\n",
    "\n",
    "        df[\"falencia\"] = 0\n",
    "        df.loc[df[\"mbp_baixa\"] | (df[\"vasopressor_ativo\"] & df[\"lactato_alto\"]), \"falencia\"] = 1\n",
    "\n",
    "        df[\"falencia_onset\"] = 0\n",
    "        for stay_id, sub in df.groupby(\"stay_id\"):\n",
    "            idx = sub.index[sub[\"falencia\"] == 1]\n",
    "            if len(idx):\n",
    "                df.loc[idx[0], \"falencia_onset\"] = 1\n",
    "\n",
    "        parts.append(df[[\"stay_id\", \"charttime\", \"falencia\", \"falencia_onset\"]])\n",
    "        total += len(df)\n",
    "\n",
    "    df_out = pd.concat(parts, ignore_index=True)\n",
    "    df_out.to_parquet(out_path, index=False)\n",
    "    print(f\"Labels salvos em {out_path} ({len(df_out)} linhas).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2698a9d6",
   "metadata": {},
   "source": [
    "## 5) Preprocess step: feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac37620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extrai features derivadas estilo CircularyFailure (n_meas/min/max/mean/instab/intens/cumul) por stay.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def derive_features_for_stay(sub: pd.DataFrame, feature_cols: List[str], window: int = 12) -> pd.DataFrame:\n",
    "    sub = sub.sort_values(\"charttime\")\n",
    "    data = {\n",
    "        \"stay_id\": sub[\"stay_id\"].values,\n",
    "        \"charttime\": sub[\"charttime\"].values,\n",
    "    }\n",
    "    for col in feature_cols:\n",
    "        s = sub[col]\n",
    "        data[f\"n_meas_{col}\"] = s.notna().cumsum().values\n",
    "        data[f\"min_{col}\"] = s.expanding().min().values\n",
    "        data[f\"max_{col}\"] = s.expanding().max().values\n",
    "        data[f\"mean_{col}\"] = s.expanding().mean().values\n",
    "        data[f\"{col}_instab\"] = s.rolling(window=window, min_periods=1).std().values\n",
    "        data[f\"{col}_intens\"] = s.diff().abs().values\n",
    "        data[f\"{col}_cumul\"] = s.fillna(0).cumsum().values\n",
    "        data[col] = s.values\n",
    "    out = pd.DataFrame(data)\n",
    "    return out\n",
    "\n",
    "\n",
    "def process_part(part_path: Path, labels_df: pd.DataFrame, out_dir: Path, window: int = 12):\n",
    "    df = pd.read_parquet(part_path)\n",
    "    df[\"charttime\"] = pd.to_datetime(df[\"charttime\"])\n",
    "    # só gera features para vitais/labs; exclui targets/estáticos e flags imputed\n",
    "    skip = {\"stay_id\", \"charttime\", \"falencia\", \"falencia_onset\", \"mortality\", \"vasopressor_ativo\", \"rel_minutes\"}\n",
    "    feature_cols = [c for c in df.columns if c not in skip and not c.endswith(\"_imputed\")]\n",
    "    imputed_cols = [c for c in df.columns if c.endswith(\"_imputed\")]\n",
    "    stay_ids = df[\"stay_id\"].unique()\n",
    "    lbl_sub = labels_df[labels_df[\"stay_id\"].isin(stay_ids)]\n",
    "\n",
    "    results = []\n",
    "    for stay_id, sub in tqdm(df.groupby(\"stay_id\"), desc=f\"{part_path.name}\", leave=False):\n",
    "        feats = derive_features_for_stay(sub, feature_cols, window=window)\n",
    "        lbl = lbl_sub[lbl_sub[\"stay_id\"] == stay_id][[\"charttime\", \"falencia\", \"falencia_onset\"]]\n",
    "        feats = feats.merge(lbl, on=\"charttime\", how=\"left\")\n",
    "        # mantem mortalidade e vasopressor_ativo (nao geram features derivadas)\n",
    "        feats = feats.merge(sub[[\"charttime\", \"mortality\"]].drop_duplicates(\"charttime\"), on=\"charttime\", how=\"left\")\n",
    "        feats = feats.merge(sub[[\"charttime\", \"vasopressor_ativo\"]].drop_duplicates(\"charttime\"), on=\"charttime\", how=\"left\")\n",
    "        # mantem colunas _imputed para inspecao\n",
    "        if imputed_cols:\n",
    "            feats = feats.merge(sub[[\"charttime\"] + imputed_cols], on=\"charttime\", how=\"left\")\n",
    "        feats = feats.fillna(0)\n",
    "        results.append(feats)\n",
    "\n",
    "    out = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # Reordena colunas para deixar flags _imputed ao lado da respectiva variável e remover rel_minutes se vier\n",
    "    ordered: List[str] = []\n",
    "    for base in (\"stay_id\", \"charttime\", \"falencia\", \"falencia_onset\", \"mortality\", \"vasopressor_ativo\"):\n",
    "        if base in out.columns:\n",
    "            ordered.append(base)\n",
    "        if base == \"vasopressor_ativo\":\n",
    "            imp = \"vasopressor_ativo_imputed\"\n",
    "            if imp in out.columns:\n",
    "                ordered.append(imp)\n",
    "    for col in feature_cols:\n",
    "        for derived in (\n",
    "            f\"n_meas_{col}\",\n",
    "            f\"min_{col}\",\n",
    "            f\"max_{col}\",\n",
    "            f\"mean_{col}\",\n",
    "            f\"{col}_instab\",\n",
    "            f\"{col}_intens\",\n",
    "            f\"{col}_cumul\",\n",
    "            col,\n",
    "        ):\n",
    "            if derived in out.columns:\n",
    "                ordered.append(derived)\n",
    "        imp = f\"{col}_imputed\"\n",
    "        if imp in out.columns:\n",
    "            ordered.append(imp)\n",
    "    for col in out.columns:\n",
    "        if col not in ordered and col != \"rel_minutes\":\n",
    "            ordered.append(col)\n",
    "    out = out[ordered]\n",
    "\n",
    "    out_path = out_dir / part_path.name.replace(\".parquet\", \"_features.parquet\")\n",
    "    out.to_parquet(out_path, index=False)\n",
    "    # salva amostra CSV para inspeção\n",
    "    sample_path = out_dir / part_path.name.replace(\".parquet\", \"_features_amostra.csv\")\n",
    "    out.head(200).to_csv(sample_path, index=False)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def fe_extract_main():\n",
    "    parser = argparse.ArgumentParser(description=\"Extrai features derivadas por stay dos imputed parts.\")\n",
    "    parser.add_argument(\"--imputed-dir\", type=str, required=True, help=\"Diretório com imputed parts (parquet).\")\n",
    "    parser.add_argument(\"--labels\", type=str, required=True, help=\"Parquet de labels (charttime, falencia, falencia_onset).\")\n",
    "    parser.add_argument(\"--out-dir\", type=str, default=config.OUTPUT_DIR / \"preprocess/batches/features_raw\", help=\"Diretório de saída.\")\n",
    "    parser.add_argument(\"--window\", type=int, default=12, help=\"Janela (n passos) para instabilidade (std rolling).\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    imp_dir = Path(args.imputed_dir)\n",
    "    out_dir = Path(args.out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    labels_df = pd.read_parquet(args.labels)\n",
    "    labels_df[\"charttime\"] = pd.to_datetime(labels_df[\"charttime\"])\n",
    "\n",
    "    parts = sorted(imp_dir.glob(\"*.parquet\"))\n",
    "    if not parts:\n",
    "        raise SystemExit(f\"Nenhum parquet encontrado em {imp_dir}\")\n",
    "\n",
    "    for p in tqdm(parts, desc=\"Parts\"):\n",
    "        process_part(p, labels_df, out_dir, window=args.window)\n",
    "\n",
    "    print(f\"Features salvas em {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8cc63",
   "metadata": {},
   "source": [
    "## 6) Build HDF5 from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40411b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Constrói H5 a partir de features derivadas (partes) + labels.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import gc\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def create_groups(h5: tables.File, n_features: int):\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        h5.create_earray(\"/data\", split, atom=tables.Float32Atom(), shape=(0, n_features))\n",
    "        h5.create_earray(\"/labels\", split, atom=tables.Float32Atom(), shape=(0, 1))\n",
    "        h5.create_earray(\"/patient_windows\", split, atom=tables.Int32Atom(), shape=(0, 3))\n",
    "        h5.create_earray(\"/patient_windows\", f\"{split}_stay_ids\", atom=tables.Int32Atom(), shape=(0,))\n",
    "\n",
    "\n",
    "def build_h5_features_main():\n",
    "    parser = argparse.ArgumentParser(description=\"Cria dataset HDF5 a partir de features derivadas + labels.\")\n",
    "    parser.add_argument(\"--features-dir\", type=str, required=True, help=\"Diretório com *_features.parquet.\")\n",
    "    parser.add_argument(\"--out\", type=str, default=config.OUTPUT_DIR / \"preprocess\" / \"h5\" / \"dataset_features.h5\")\n",
    "    parser.add_argument(\"--split-path\", type=str, default=None, help=\"TSV opcional stay_id/split.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    feat_dir = Path(args.features_dir)\n",
    "    parts = sorted(feat_dir.glob(\"*_features.parquet\"))\n",
    "    if not parts:\n",
    "        raise SystemExit(f\"Nenhum *_features.parquet em {feat_dir}\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Determina feature_cols (exclui targets, mortalidade e antropometria/derivados para evitar leak)\n",
    "    sample = pd.read_parquet(parts[0])\n",
    "    drop_bases = {\"bmi\", \"height\", \"weight\"}\n",
    "\n",
    "    def is_drop(col: str) -> bool:\n",
    "        if col in {\"stay_id\", \"charttime\", \"falencia\", \"falencia_onset\", \"mortality\"}:\n",
    "            return True\n",
    "        for base in drop_bases:\n",
    "            if (\n",
    "                col == base\n",
    "                or col == f\"{base}_imputed\"\n",
    "                or col.startswith(f\"n_meas_{base}\")\n",
    "                or col.startswith(f\"min_{base}\")\n",
    "                or col.startswith(f\"max_{base}\")\n",
    "                or col.startswith(f\"mean_{base}\")\n",
    "                or col.startswith(f\"{base}_instab\")\n",
    "                or col.startswith(f\"{base}_intens\")\n",
    "                or col.startswith(f\"{base}_cumul\")\n",
    "            ):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    feature_cols = [c for c in sample.columns if not is_drop(c)]\n",
    "    n_features = len(feature_cols)\n",
    "\n",
    "    out_h5 = Path(args.out)\n",
    "    out_h5.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_h5.exists():\n",
    "        out_h5.unlink()\n",
    "\n",
    "    # Split\n",
    "    all_stays = pd.concat([pd.read_parquet(p, columns=[\"stay_id\"]).drop_duplicates() for p in parts])[\"stay_id\"].unique()\n",
    "    if args.split_path and Path(args.split_path).exists():\n",
    "        split_df = pd.read_csv(args.split_path, sep=\"\\t\")\n",
    "        split_map = dict(zip(split_df[\"stay_id\"], split_df[\"split\"]))\n",
    "    else:\n",
    "        train_ids, test_ids = train_test_split(all_stays, test_size=0.15, random_state=42)\n",
    "        train_ids, val_ids = train_test_split(train_ids, test_size=0.1765, random_state=42)\n",
    "        split_map = {sid: \"train\" for sid in train_ids}\n",
    "        split_map.update({sid: \"val\" for sid in val_ids})\n",
    "        split_map.update({sid: \"test\" for sid in test_ids})\n",
    "\n",
    "    with tables.open_file(out_h5, mode=\"w\") as h5:\n",
    "        h5.create_group(\"/\", \"data\", \"features\")\n",
    "        h5.create_group(\"/\", \"labels\", \"labels\")\n",
    "        h5.create_group(\"/\", \"patient_windows\", \"windows\")\n",
    "        create_groups(h5, n_features)\n",
    "\n",
    "        total_stays = 0\n",
    "        for part in tqdm(parts, desc=\"Parts\"):\n",
    "            df = pd.read_parquet(part)\n",
    "            df = df.sort_values([\"stay_id\", \"charttime\"])\n",
    "            df[feature_cols] = df[feature_cols].fillna(0).astype(np.float32)\n",
    "            # normalização simples z-score por coluna\n",
    "            means = df[feature_cols].mean()\n",
    "            stds = df[feature_cols].std().replace(0, 1.0)\n",
    "            df[feature_cols] = (df[feature_cols] - means) / stds\n",
    "            df[\"falencia\"] = df[\"falencia\"].fillna(0).astype(np.float32)\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                sub = df[df[\"stay_id\"].map(split_map).fillna(\"train\") == split]\n",
    "                if sub.empty:\n",
    "                    continue\n",
    "                data_arr = h5.get_node(f\"/data/{split}\")\n",
    "                label_arr = h5.get_node(f\"/labels/{split}\")\n",
    "                window_arr = h5.get_node(f\"/patient_windows/{split}\")\n",
    "                stay_ids_arr = h5.get_node(f\"/patient_windows/{split}_stay_ids\")\n",
    "                for stay_id, sid_df in sub.groupby(\"stay_id\"):\n",
    "                    start_idx = data_arr.nrows\n",
    "                    data_arr.append(sid_df[feature_cols].to_numpy())\n",
    "                    label_arr.append(sid_df[[\"falencia\"]].to_numpy())\n",
    "                    stop_idx = data_arr.nrows\n",
    "                    window_arr.append(np.array([[start_idx, stop_idx, int(stay_id)]], dtype=np.int32))\n",
    "                    stay_ids_arr.append(np.array([int(stay_id)], dtype=np.int32))\n",
    "                    total_stays += 1\n",
    "            del df\n",
    "            gc.collect()\n",
    "        h5.set_node_attr(\"/\", \"feature_names\", feature_cols)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"H5 salvo em {out_h5} com {n_features} features derivadas, {total_stays} stays. Tempo: {elapsed/60:.1f} min.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ca35f",
   "metadata": {},
   "source": [
    "## 7) Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d779e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) prep_merge\n",
    "import sys\n",
    "\n",
    "sys.argv = [\"prep_merge\", \"--base-dir\", str(BASE_DIR), \"--out-dir\", str(PREPROCESS_DIR)]\n",
    "prep_merge_main()\n",
    "\n",
    "# 2) impute\n",
    "merged_path = PREPROCESS_DIR / \"merged.parquet\"\n",
    "static_path = PREPROCESS_DIR / \"static.parquet\"\n",
    "\n",
    "sys.argv = [\n",
    "    \"impute\",\n",
    "    \"--merged\", str(merged_path),\n",
    "    \"--static\", str(static_path),\n",
    "    \"--out\", str(IMPUTED_DIR),\n",
    "    \"--grid-minutes\", str(GRID_MINUTES),\n",
    "]\n",
    "impute_main()\n",
    "\n",
    "# 3) labels\n",
    "sys.argv = [\"label\", \"--imputed\", str(IMPUTED_DIR), \"--out\", str(LABELS_PATH)]\n",
    "label_main()\n",
    "\n",
    "# 4) feature extraction\n",
    "sys.argv = [\n",
    "    \"fe_extract\",\n",
    "    \"--imputed-dir\", str(IMPUTED_DIR),\n",
    "    \"--labels\", str(LABELS_PATH),\n",
    "    \"--out-dir\", str(FEATURES_DIR),\n",
    "    \"--window\", \"12\",\n",
    "]\n",
    "fe_extract_main()\n",
    "\n",
    "# 5) build HDF5\n",
    "sys.argv = [\n",
    "    \"build_h5\",\n",
    "    \"--features-dir\", str(FEATURES_DIR),\n",
    "    \"--out\", str(H5_PATH),\n",
    "]\n",
    "build_h5_features_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c663ab",
   "metadata": {},
   "source": [
    "## Build HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cca04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build HDF5 (NWICU)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables\n",
    "\n",
    "BATCH_DIR = Path(OUT_DIR) / \"preprocess\" / \"batches\" / \"features_filtered\"\n",
    "H5_PATH = Path(OUT_DIR) / \"preprocess\" / \"h5\" / \"dataset_features.h5\"\n",
    "SPLIT_TSV = Path(OUT_DIR) / \"split_all.tsv\"\n",
    "\n",
    "ID_COL = \"stay_id\"\n",
    "TIME_COL = \"charttime\"\n",
    "LABEL_COL = \"falencia\"\n",
    "SEED = 42\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC = 0.1\n",
    "\n",
    "# --- helpers ---\n",
    "def ensure_groups(h5, groups):\n",
    "    for g in groups:\n",
    "        if f\"/{g}\" not in h5:\n",
    "            h5.create_group(\"/\", g, f\"{g} group\")\n",
    "\n",
    "\n",
    "def create_earray(h5, path, atom, n_cols, expectedrows=10_000_000):\n",
    "    parent, name = path.rsplit(\"/\", 1)\n",
    "    return h5.create_earray(parent, name, atom=atom, shape=(0, n_cols), expectedrows=expectedrows)\n",
    "\n",
    "\n",
    "def build_split(stay_ids, seed, train_frac, val_frac):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    stay_ids = np.array(sorted(stay_ids), dtype=int)\n",
    "    rng.shuffle(stay_ids)\n",
    "    n = len(stay_ids)\n",
    "    n_train = int(n * train_frac)\n",
    "    n_val = int(n * val_frac)\n",
    "    return {\n",
    "        \"train\": set(stay_ids[:n_train].tolist()),\n",
    "        \"val\": set(stay_ids[n_train:n_train + n_val].tolist()),\n",
    "        \"test\": set(stay_ids[n_train + n_val:].tolist()),\n",
    "    }\n",
    "\n",
    "\n",
    "def load_split_map(split_tsv, stay_ids, seed, train_frac, val_frac):\n",
    "    if split_tsv.exists():\n",
    "        df = pd.read_csv(split_tsv, sep=\"\t\")\n",
    "        if {ID_COL, \"split\"}.issubset(df.columns):\n",
    "            return dict(zip(df[ID_COL].astype(int), df[\"split\"]))\n",
    "    split_sets = build_split(stay_ids, seed, train_frac, val_frac)\n",
    "    split_tsv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with split_tsv.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{ID_COL}\\tsplit\\n\")\n",
    "        for split, ids in split_sets.items():\n",
    "            for sid in sorted(ids):\n",
    "                f.write(f\"{sid}\\t{split}\\n\")\n",
    "    return {sid: split for split, ids in split_sets.items() for sid in ids}\n",
    "\n",
    "\n",
    "paths = sorted(p for p in BATCH_DIR.glob(\"batch_*.parquet\") if not p.name.endswith(\"_labels.parquet\"))\n",
    "if not paths:\n",
    "    raise SystemExit(f\"No batch_*.parquet found in {BATCH_DIR}\")\n",
    "\n",
    "stay_ids = set()\n",
    "for p in paths:\n",
    "    df_ids = pd.read_parquet(p, columns=[ID_COL])\n",
    "    stay_ids.update(df_ids[ID_COL].dropna().astype(int).unique().tolist())\n",
    "\n",
    "split_map = load_split_map(SPLIT_TSV, stay_ids, SEED, TRAIN_FRAC, VAL_FRAC)\n",
    "\n",
    "first_df = pd.read_parquet(paths[0])\n",
    "drop_cols = {ID_COL, TIME_COL, LABEL_COL}\n",
    "feature_cols = [c for c in first_df.columns if c not in drop_cols]\n",
    "\n",
    "H5_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "if H5_PATH.exists():\n",
    "    H5_PATH.unlink()\n",
    "\n",
    "with tables.open_file(H5_PATH, mode=\"w\") as h5:\n",
    "    ensure_groups(h5, [\"data\", \"labels\", \"patient_windows\"])\n",
    "    data_arrays = {}\n",
    "    label_arrays = {}\n",
    "    window_arrays = {}\n",
    "    stay_id_arrays = {}\n",
    "\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        data_arrays[split] = create_earray(h5, f\"/data/{split}\", tables.Float32Atom(), n_cols=len(feature_cols))\n",
    "        label_arrays[split] = create_earray(h5, f\"/labels/{split}\", tables.Float32Atom(), n_cols=1)\n",
    "        window_arrays[split] = create_earray(h5, f\"/patient_windows/{split}\", tables.Int32Atom(), n_cols=3)\n",
    "        stay_id_arrays[split] = h5.create_earray(\"/patient_windows\", f\"{split}_stay_ids\", atom=tables.Int32Atom(), shape=(0,))\n",
    "\n",
    "    for idx, path in enumerate(paths, start=1):\n",
    "        df = first_df if idx == 1 else pd.read_parquet(path)\n",
    "        if TIME_COL in df.columns:\n",
    "            df = df.sort_values([ID_COL, TIME_COL])\n",
    "        else:\n",
    "            df = df.sort_values([ID_COL])\n",
    "        df[ID_COL] = df[ID_COL].astype(int)\n",
    "\n",
    "        for stay_id, df_sid in df.groupby(ID_COL, sort=False):\n",
    "            split = split_map.get(int(stay_id), \"train\")\n",
    "            d_arr = data_arrays[split]\n",
    "            l_arr = label_arrays[split]\n",
    "            w_arr = window_arrays[split]\n",
    "            sid_arr = stay_id_arrays[split]\n",
    "\n",
    "            start = d_arr.nrows\n",
    "            feat = df_sid[feature_cols].astype(np.float32).to_numpy()\n",
    "            lbl = df_sid[[LABEL_COL]].astype(np.float32).to_numpy() if LABEL_COL in df_sid.columns else np.zeros((len(df_sid), 1), dtype=np.float32)\n",
    "            d_arr.append(feat)\n",
    "            l_arr.append(lbl)\n",
    "            stop = d_arr.nrows\n",
    "\n",
    "            w_arr.append(np.array([[start, stop, int(stay_id)]], dtype=np.int32))\n",
    "            sid_arr.append(np.array([int(stay_id)], dtype=np.int32))\n",
    "\n",
    "        if idx % 10 == 0 or idx == 1 or idx == len(paths):\n",
    "            print(f\"[{idx}/{len(paths)}] {path.name}\")\n",
    "\n",
    "    h5.set_node_attr(\"/\", \"feature_names\", feature_cols)\n",
    "    h5.set_node_attr(\"/\", \"label_column\", LABEL_COL)\n",
    "\n",
    "print(\"H5 saved to\", H5_PATH, \"with\", len(feature_cols), \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build per-stay falencia summary (falencia_normal + 45/60min + mortality)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "PREPROCESS_DIR = OUT_DIR / \"preprocess\"\n",
    "INPUT_DIR = PREPROCESS_DIR / \"batches\" / \"features_filtered\"\n",
    "OUT_CSV = PREPROCESS_DIR / \"falencia_stay_summary.csv\"\n",
    "MORTALITY_PATH = PREPROCESS_DIR / \"mortality_by_stay.csv\"\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    str(Path(\"src/clustering/minirocket/pipelines/build_falencia_stay_summary.py\")),\n",
    "    \"--input_dir\", str(INPUT_DIR),\n",
    "    \"--mortality_path\", str(MORTALITY_PATH),\n",
    "    \"--out_csv\", str(OUT_CSV),\n",
    "    \"--time_col\", \"charttime\",\n",
    "    \"--id_col\", \"stay_id\",\n",
    "    \"--mbp_col\", \"mbp\",\n",
    "    \"--vaso_col\", \"vasopressor_ativo\",\n",
    "    \"--lactate_col\", \"lab_50813\",\n",
    "    \"--falencia_col\", \"falencia\",\n",
    "    \"--step_min_45\", \"5\", \"--window_min_45\", \"45\",\n",
    "    \"--step_min_60\", \"60\", \"--window_min_60\", \"60\",\n",
    "]\n",
    "print(\" \".join(cmd))\n",
    "subprocess.run(cmd, check=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}